> **摘要**：在这期深度访谈中，Andrej Karpathy 主要讨论了通用人工智能（AGI）和大型语言模型（LLMs）的未来与演变。卡帕西认为，这一十年将是 “智能体十年”，强调在解决当前人工智能认知缺陷方面仍有大量工作要做，比如缺乏持续学习和多模态技术。他回顾了从 AlexNet 到早期强化学习的 AI 发展历史，并批评当前强化学习方法的局限性，指出它们过于嘈杂，称之为 “用吸管吸收监督”。 对话还涉及人工智能辅助编程的挑战，现有的大语言模型在智力上、非模板化任务中挣扎，比如构建他的纳米聊天项目，并探讨了他对 “尤里卡 ”的教育愿景，旨在通过打造完美定制的“知识阶梯” 来革新学习，以最大化学生理解。。以及很多内容，详见下文。

**采访者：**
今天我和 Andrej Karpathy 在一起。Andrej，你为什么说这将是“智能体（Agents）的十年”，而不是“智能体的元年”？首先，谢谢你邀请我，很高兴来到这里。

## 智能体的十年 vs 智能体的元年

**Andrej Karpathy：**
你刚才提到的这句话——“这是智能体的十年”——实际上是对之前一个说法的回应。我不确定是谁说的，但他们暗示今年是 LLM（大语言模型）及其演变背景下的“智能体元年”。

我对这个说法有点意见，因为我觉得行业里存在一些过度预测的现象。在我看来，更准确的描述应该是“智能体的十年”。虽然我们已经有一些非常早期的智能体令人印象深刻，比如 Claude 和 Codex 等等，我也每天都在用，但我仍然觉得还有大量的工作要做。

我的反应是，我们将与这些东西共处十年。它们会变得更好，这将会非常美妙。

**采访者：**
我只是对这个时间线的含义做出了反应。你认为什么事情需要花十年才能完成？瓶颈是什么？

**Andrej Karpathy：**
实际上就是让它真正运作起来。当你谈论一个智能体，或者实验室里的人所设想的，甚至可能是我所设想的，你应该把它想象成一个你会雇佣来和你一起工作的员工或实习生。

比如，你这里有一些员工。你什么时候会愿意让像 Claude 或 Codex 这样的智能体来做那些工作？目前，它们显然还做不到。要让它们能够胜任需要什么条件？为什么你今天不这么做？

你今天不这么做的原因是因为它们根本还没法那样工作。它们的智能程度还不够，多模态能力不足，不能操作电脑，诸如此类。它们做不到你刚才提到的很多事情。它们没有持续学习的能力。你不能只告诉它们一件事，然后指望它们能记住。它们在认知上还有缺陷，就是还没法用。要解决所有这些问题大约需要十年时间。

**采访者：**
很有趣。作为一个职业播客主持人和一个远距离观察 AI 的人，我很容易识别出缺少什么：缺少持续学习，或者缺少多模态能力。但我真的没有很好的方法来给它设定一个时间表。

如果有人问持续学习需要多久，我完全不知道这是一个应该花 5 年、10 年还是 50 年的项目。为什么是十年？为什么不是一年？为什么不是 50 年？

**Andrej Karpathy：**
这就涉及到我的一些直觉，以及根据我在该领域的经验进行的一些推断。我在 AI 领域已经快二十年了。大概有 15 年左右，不算太长。你采访过 Richard Sutton，他在这个领域的时间要长得多。我有大约 15 年的经验，看着人们做出预测，然后看结果如何。

我也在业界待了一段时间，我在研究部门待过，也在工业界工作过。我从中获得了一种普遍的直觉。我觉得这些问题是可以解决的，是可以克服的，但它们仍然很困难。如果我把这些因素平均一下，对我来说感觉就像是十年。

## AI 发展的历史回顾：从 AlexNet 到早期智能体

**采访者：**
这很有趣。我不仅想听听历史，还想听听在各个不同的突破时刻，身处其中的人们觉得即将发生什么。在哪些方面他们的感觉是过度悲观或过度乐观的？我们要不要一个个回顾一下？

**Andrej Karpathy：**
这是一个巨大的问题，因为你在谈论过去 15 年发生的事情。AI 之所以如此奇妙，是因为发生过几次剧烈的转变，整个领域突然间看起来完全不同了。我大概经历了两三次这样的转变。我依然认为这种转变还会继续发生，因为它们似乎以惊人的规律性出现。

当我的职业生涯开始时，当我开始研究深度学习，或者说对深度学习产生兴趣时，这纯粹是因为我在多伦多大学正好在 Geoff Hinton 隔壁。Geoff Hinton 当然是 AI 的教父级人物。他当时正在训练所有这些神经网络。我觉得这太不可思议、太有趣了。但当时这绝不是 AI 领域里大家都在做的主流事情。这只是边缘的一个小众课题。

这也许是随 AlexNet 等出现的第一次剧烈的地震式转变。AlexNet 重新定位了所有人，大家都开始训练神经网络，但当时仍然是非常针对具体任务的。也许我有一个图像分类器，或者我有一个神经机器翻译器之类的。

人们开始非常缓慢地对智能体产生兴趣。人们开始想，“好吧，也许我们在视觉皮层之类的地方打了个勾，但大脑的其他部分呢？我们如何获得一个完整的智能体或一个可以在世界上交互的完整实体？”

大约在 2013 年左右的 Atari 深度强化学习转变，在我看来是早期智能体努力的一部分，因为它试图让智能体不仅能感知世界，还能采取行动、进行交互并从环境中获得奖励。当时，用的是 Atari 游戏。我觉得那是走了一步弯路。

那是一个弯路，甚至我参与的早期 OpenAI 也采纳了这种做法，因为当时的时代精神就是强化学习环境、游戏、玩游戏、打通关、搞定各种不同类型的游戏，OpenAI 做了很多这样的工作。那是 AI 的另一个显赫时期，可能两三年或四年里，每个人都在游戏上做强化学习。那都有点走偏了。

我在 OpenAI 试图做的是，我一直有点怀疑游戏是否是通向 AGI（通用人工智能）的途径。因为在我看来，你想要的是像会计师那样的东西，或者是能与现实世界交互的东西。我就是看不出游戏怎么能累积成那样。

例如，我在 OpenAI 的项目属于 Universe 项目的范畴，是一个使用键盘和鼠标操作网页的智能体。我真的想拥有一个能与实际数字世界交互、能做知识工作的智能体。

结果证明这太早了，实在太早了，早到我们根本不应该在那上面花时间。因为如果你只是在环境中跌跌撞撞、乱敲键盘、乱点鼠标，试图获得奖励，你的奖励太稀疏了，你根本学不到东西。你会烧掉一大片森林的算力，却永远无法让项目起飞。

你缺少的是神经网络中的表征能力。例如，今天人们在训练那些使用电脑的智能体，但他们是在大语言模型的基础上做的。你必须先有语言模型，必须先有表征，你必须通过所有的预训练和 LLM 的东西来做到这一点。

我觉得大概来说，人们有好几次都试图过早地实现完整的东西，人们真的试图过早地去追求智能体，我想说。那就是 Atari 和 Universe，甚至是我自己的经历。实际上你必须先做一些事情，然后才能做那些智能体。现在智能体的能力强多了，但也许我们仍然缺少那个技术栈的某些部分。

我想说这就是人们当时做的三大类事情：针对特定任务训练神经网络，尝试第一轮智能体，然后可能是 LLM 以及在附加其他所有东西之前先寻求神经网络的表征能力。

## 进化、预训练与“动物”类比

**采访者：**
有趣。如果我要为 Sutton 的观点辩护（Steelman），那就是人类可以一次性承担所有事情，或者甚至动物也可以一次性承担所有事情。动物也许是更好的例子，因为它们甚至没有语言的脚手架。它们只是被扔进这个世界，必须在没有任何标签的情况下理解一切。

那么 AGI 的愿景应该就是某种东西，它观察感官数据，看电脑屏幕，然后从零开始弄清楚发生了什么。如果把一个人放在类似的情况下，必须从头开始训练……这就像人类成长或动物成长一样。为什么这就不能是 AI 的愿景，而不是我们现在做的这种经过数百万年数据训练的东西？

**Andrej Karpathy：**
这是一个非常好的问题。Sutton 参加过你的播客，我看了那期节目，我还写了一篇关于那期播客的文章，里面涉及了一些我的看法。

我对用动物做类比非常谨慎，因为它们是通过一个非常不同的优化过程产生的。动物是进化而来的，它们自带大量的硬件。例如，我在文章中举的例子是斑马。斑马出生几分钟后就能到处跑并跟随它的母亲。这是一件极其复杂的事情。这不是强化学习。这是某种硬编码的东西。

进化显然有某种方式将我们神经网络的权重编码在 ATCG（DNA碱基）中，我完全不知道那是怎么运作的，但它显然是有效的。大脑来自一个非常不同的过程，我很犹豫是否要从中汲取灵感，因为我们实际上并没有运行那个过程。

在我的文章中，我说我们不是在制造动物。我们在制造幽灵或灵魂，或者不管人们想叫它什么，因为我们不是通过进化来训练的。我们是通过模仿人类以及人类在互联网上留下的数据来训练的。你最终得到的是这些空灵的精神实体，因为它们完全是数字化的，而且它们在模仿人类。

这是一种不同类型的智能。如果你想象一个智能空间，我们的起点几乎完全不同。我们真的不是在制造动物。但也确实有可能随着时间的推移让它们更像动物一点，我认为我们应该这样做。

还有一点。我觉得 Sutton 有一个非常……他的框架是，“我们要制造动物。”我认为如果我们能做到这一点，那就太棒了。那将是惊人的。如果有一种单一的算法，你可以直接在互联网上运行它，它就能学会一切，那将是不可思议的。

我不确定这种算法是否存在，而且这肯定不是动物所做的，因为动物有进化的外循环。很多看起来像学习的过程其实更像是大脑的成熟过程。我认为动物进行的强化学习非常少。很多强化学习更像是运动任务，而不是智力任务。所以我实际上有点认为人类并不真正使用 RL（强化学习），粗略地说。

**采访者：**
你能重复一下最后一句吗？很多智力不是运动任务……那是怎么回事，抱歉？

**Andrej Karpathy：**
在我看来，很多强化学习更像是运动类的、简单的任务，比如投篮。但我不认为人类在很多智力任务（如解决问题等）上使用强化学习。这并不意味着我们不应该在研究中这样做，我只是觉得这才是动物做或不做的事情。

**采访者：**
我要花点时间消化一下，因为这里有很多不同的想法。我可以问一个澄清性的问题来理解这个观点。你暗示进化正在做那种预训练所做的事情，即构建某种能够理解世界的实体。

区别在于，在人类的情况下，进化必须通过 3GB 的 DNA 进行滴定。这与模型的权重非常不同。从字面上看，模型的权重就是一个大脑，这显然不存在于精子和卵子中。所以它必须被生长出来。此外，大脑中每一个突触的信息根本不可能存在于 DNA 的 3GB 数据中。

进化似乎更接近于找到那个随后进行终身学习的算法。现在，正如你所说，也许终身学习并不类同于 RL。这与你刚才说的是否兼容，还是你会反对这个观点？

**Andrej Karpathy：**
我想是的。我同意你的观点，这里发生了一些奇迹般的压缩，因为显然神经网络的权重并没有存储在 ATCG 中。这里有某种巨大的压缩。有一些学习算法被编码进去了，它们接管并进行一些在线学习。在这点上我绝对同意你。

我想说我更注重实际。我不是从“让我们制造动物”的角度出发的。我是从“让我们制造有用的东西”的角度出发的。我戴着安全帽，我只是观察到我们不会去做进化，因为我不知道该怎么做。

但事实证明，我们可以通过模仿互联网文档来构建这些幽灵、类似灵魂的实体。这行得通。这是一种让你达到某种具有大量内置知识和智能起点的途径，在某种程度上类似于进化所做的事情。这就是为什么我称预训练为这种“蹩脚的进化”。这是利用我们的技术和现有资源实际上可能做到的版本，以此作为一个起点，然后我们可以做强化学习之类的事情。

**采访者：**
为了给另一种观点辩护，在做完这次 Sutton 访谈并思考了一番之后，我觉得他在这里有一个重要的观点。进化并没有真正给我们知识。它给了我们寻找知识的算法，这似乎与预训练不同。

也许这个观点是，预训练有助于构建那种可以更好地学习的实体。它教会了元学习（meta-learning），因此它类似于寻找算法。但如果是“进化给我们知识，预训练给我们知识”，这个类比似乎就站不住脚了。

**Andrej Karpathy：**
这很微妙，我认为你反驳得对，但基本上预训练所做的事情是，你在互联网上进行下一个 token 的预测训练，并将其训练成一个神经网络。它在做两件不相关的事情。

第一，它在获取所有这些知识，正如我所说的。第二，它实际上正在变得智能。通过观察互联网上的算法模式，它在神经网络内部启动了所有这些微小的电路和算法，来做像上下文学习（in-context learning）之类的事情。

你不需要也不想要那些知识。我认为这可能会在整体上拖累神经网络，因为它有时会让它们过度依赖知识。例如，我觉得智能体不太擅长的一件事就是脱离互联网上现有的数据流形（data manifold）。如果它们的知识少一点或者记忆少一点，也许它们会表现得更好。

我认为我们未来要做的事情——这将是研究范式的一部分——是找出去除部分知识并保留我所说的“认知核心”的方法。这是一个剥离了知识但包含算法、包含智能和解决问题的魔法、策略以及所有这些东西的智能实体。

## 上下文学习与梯度下降

**采访者：**
这里有太多有趣的东西了。让我们从上下文学习开始。这是一个显而易见的点，但我认为值得明确地说出来并思考一下。

这些模型看起来最智能的情况——也就是我和它们交谈时会觉得“哇，对面真的有个东西在思考并回应我”的情况——是如果它犯了一个错误，它会说，“哦，等等，那样想是错的。我重来。”

所有这些都发生在上下文中。这就是我觉得你能看到的真正的智能所在。这种上下文学习过程是通过预训练的梯度下降发展出来的。它自发地元学习了上下文学习，但上下文学习本身并不是梯度下降，就像我们作为人类能够做事的终身智能受进化的制约，但我们有生之年的学习是通过其他过程发生的一样。

**Andrej Karpathy：**
我不完全同意这一点，但你应该继续你的想法。

**采访者：**
好吧，我很好奇那个类比是如何被打破的。

**Andrej Karpathy：**
我很犹豫是否该说上下文学习不是在做梯度下降。它不是在做显式的梯度下降。上下文学习是 token 窗口内的模式补全。事实证明互联网上有大量的模式。

你是对的，模型学会了补全模式，这存在于权重之中。神经网络的权重试图发现模式并补全模式。神经网络内部发生了一些适应，这很神奇，它仅仅因为有大量模式就从互联网数据中自然产生了。

但我得说，有一些论文我觉得很有趣，它们研究了上下文学习背后的机制。我确实认为上下文学习有可能在神经网络层内部运行一个小的梯度下降循环。

我特别记得有一篇论文，他们在用上下文学习做线性回归。你输入神经网络的是 XY 对，XY, XY, XY，它们恰好在一条线上。然后你输入 X，期望得到 Y。当你这样训练神经网络时，它实际上在做线性回归。

通常当你运行线性回归时，你有一个小的梯度下降优化器，它查看 XY，查看误差，计算权重的梯度，并进行几次更新。结果证明，当他们查看那个上下文学习算法的权重时，他们发现了一些与梯度下降机制类似的类比。

事实上，我认为那篇论文甚至更有力，因为他们通过注意力机制和神经网络的所有内部结构，硬编码了一个神经网络的权重来做梯度下降。这只是我唯一的反驳。谁知道上下文学习是怎么运作的，但我认为它可能在内部做一些类似梯度下降的有趣事情。我认为这是可能的。我只是反驳你说它没有做上下文学习。谁知道它在做什么，但它可能在做类似的事情，只是我们不知道。

**采访者：**
那么这就值得思考了，如果上下文学习和预训练都在实现某种类似梯度下降的东西，为什么我们会感觉通过上下文学习我们正在接近这种持续学习、真正的智能？而你仅仅从预训练中并没有得到类似的感觉。

**Andrej Karpathy：**
你可以这么争论。如果它是同一种算法，会有什么不同呢？你可以这样思考：模型从训练中接收到的每单位信息存储了多少信息？

如果你看预训练，比如看 Llama 3，我想它是用 15 万亿个 token 训练的。如果你看那个 70B 的模型，这就相当于它在预训练中看到的每个 token 对应模型权重中的 0.07 比特信息。而在上下文学习中，如果你看 KV 缓存以及每个额外 token 如何使其增长，那是大概 320 KB。所以在模型吸收每个 token 的信息量上，这是一个 3500 万倍的差异。我想知道这是否有关联。

**Andrej Karpathy：**
我某种程度上同意。我通常这样表述：神经网络训练过程中发生的任何事情，其知识只是对训练时发生的事情的一种模糊回忆。这是因为压缩非常剧烈。你拿 15 万亿个 token，把它们压缩成只有几十亿参数的最终神经网络。显然这中间有大量的压缩。所以我称之为对互联网文档的模糊回忆。

而在神经网络的上下文窗口中发生的任何事情——你插入所有的 token 并建立所有那些 KV 缓存表征——对于神经网络来说是非常直接可访问的。所以我把 KV 缓存和测试时发生的事情比作工作记忆（working memory）。

上下文窗口中的所有东西对于神经网络来说都是非常直接可访问的。大语言模型和人类之间总是存在这些几乎令人惊讶的类比。我觉得这很令人惊讶，因为我们并不是试图直接构建人脑。我们只是发现这种方法有效，我们就这么做了。

但我确实认为，权重里的任何东西，都是对你一年前读过的东西的模糊回忆。你在测试时给它的作为上下文的任何东西，都直接在工作记忆中。这是一个非常有力的类比，可以用来思考问题。

例如，当你去问一个 LLM 关于某本书以及里面发生了什么，比如 Nick Lane 的书之类的，LLM 通常会给你一些大致正确的东西。但如果你把整个章节给它并问它问题，你会得到好得多的结果，因为它现在已经加载到模型的工作记忆中了。

说了这么长，其实我是想说我同意，这就是原因。

## 大脑类比：Transformer 缺失了什么？

**采访者：**
退一步讲，我们用这些模型最未能复制的人类智能部分是什么？

**Andrej Karpathy：**
就是很多部分。也许一种思考方式是，我不知道这是不是最好的方式，但我几乎觉得——再次强调，这些类比并不完美——我们偶然发现了 Transformer 神经网络，它非常强大，非常通用。你可以在音频、视频、文本或任何你想要的东西上训练 Transformer，它就能学习模式，而且非常强大，效果很好。

这对我来说几乎表明这就像是一块皮层组织。就像那样，因为众所周知皮层的可塑性非常强。你可以重新连接大脑的部分区域。有一些稍微有点残忍的实验，把视觉皮层重新连接到听觉皮层，动物也能学得很好，等等。

所以我认为这就是皮层组织。我认为当我们在神经网络内部进行推理和规划，做思维模型的推理痕迹时，那有点像前额叶皮层。也许这些就像是打了个小勾，但我仍然认为还有很多大脑部分和神经核团没有被探索。

例如，当我们对模型进行强化学习微调时，基底神经节在做一点强化学习。但是海马体在哪里？这还不明显。有些部分可能不重要。也许小脑对认知、思维不重要，所以也许我们可以跳过其中的一部分。

但我仍然认为，例如杏仁核，所有的情绪和本能。大脑中可能还有很多其他非常古老的神经核团，我认为我们还没有真正复制出来。我不知道我们是否应该追求构建人脑的模拟物。我本质上主要是一个工程师。

也许回答这个问题的另一种方式是，你不会雇佣这个东西当实习生。它缺少很多东西，因为它带有我们与模型交谈时都能直观感受到的认知缺陷。所以它还没完全到位。你可以把它看作并非所有的大脑部分都已就位。

## 持续学习、睡眠与记忆蒸馏

**采访者：**
这也许与思考解决这些问题需要多快有关。有时人们会谈论持续学习，“看，你可以很容易地复制这种能力。就像上下文学习作为预训练的结果自发出现一样，如果模型被激励去回忆更长时间范围内的信息，或者比单个会话更长的时间范围，那么这种更长时间范围的持续学习也会自发出现。”

所以，如果有某种包含许多会话的外循环 RL，那么这种自我微调，或者写入外部存储器之类的持续学习，就会自发出现。你认为这种事情看似合理吗？

**Andrej Karpathy：**
我只是对这有多合理没有任何先验判断。这种情况发生的可能性有多大？我不知道我是否完全认同这种观点。这些模型，当你启动它们且窗口中只有零个 token 时，它们总是从原点重新开始。所以我不知道在那个世界观里它看起来是什么样子的。

也许再做一些人类的类比——仅仅因为我觉得这大致具体且思考起来很有趣——我觉得当我醒着的时候，我在建立一天中发生的上下文窗口。但当我睡觉时，会发生一些神奇的事情，我不认为那个上下文窗口会保留下来。

有一个将其蒸馏（distillation）进我大脑权重的过程。这发生在睡眠期间等等。我们在大语言模型中没有对应的东西。对我来说，这更接近于你所说的持续学习缺失的问题。

这些模型并没有一个真正的蒸馏阶段，去获取发生的事情，强迫性地分析它，思考它，做一个合成数据生成过程，然后把它蒸馏回权重中，也许每个人有一个特定的神经网络。也许它是 LoRA。它不是全权重的神经网络。只是权重的某个小的稀疏子集被改变了。

但我们确实想创造方法来创造这些拥有非常长上下文的个体。这不仅仅是保留在上下文窗口中，因为上下文窗口会长得非常非常长。也许我们对它有某种非常精细的、稀疏的注意力机制。

但我仍然认为人类显然有某种过程将部分知识蒸馏进权重中。我们缺少这个。我也确实认为人类有某种非常精细的、稀疏的注意力机制，我认为我们开始看到这方面的一些早期迹象。

## 未来十年的算法架构预测

DeepSeek v3.2 刚刚发布，我看到他们有稀疏注意力作为一个例子，这是一种拥有非常非常长上下文窗口的方法。所以我觉得我们正在重新做进化通过非常不同的过程想出的许多认知技巧。但在认知架构上我们将殊途同归。

**采访者：**
十年后，你认为它还会是像 Transformer 这样的东西，但有着修改得多的注意力机制和更稀疏的 MLP 等等吗？

**Andrej Karpathy：**
我喜欢用时间上的平移不变性来思考这个问题。十年前，我们在哪里？2015 年。2015 年，我们主要有卷积神经网络，残差网络刚刚出来。所以非常相似，我想，但仍然有相当大的不同。Transformer 还没出现。所有这些对 Transformer 的现代调整都还没有出现。

也许我们可以押注的一些事情，我想十年后根据平移等变性，我们仍然会训练巨大的神经网络，通过前向后向传播和梯度下降进行更新，但也许它看起来有点不同，只是所有东西都大得多。

最近我回溯到了 1989 年，这对我来说是个有趣的练习，几年前，因为我在复现 Yann LeCun 1989 年的卷积网络，那是我所知的第一个通过梯度下降训练的神经网络，就像现代神经网络在数字识别上训练梯度下降一样。

我只是对如何使之现代化感兴趣。这其中有多少是算法？有多少是数据？有多少进步是算力和系统？仅仅通过穿越 33 年，我就能很快将学习误差减半。所以如果我在算法上穿越 33 年，我可以调整 Yann LeCun 在 1989 年做的事情，我可以把误差减半。

但为了获得进一步的收益，我必须增加更多的数据，我必须将训练集增加 10 倍，然后我必须增加更多的计算优化。我必须用 dropout 和其他正则化技术训练更长时间。

所以所有这些事情必须同时改进。我们可能会有更多的数据，我们可能会有更好的硬件，可能会有更好的内核和软件，可能会有更好的算法。所有这些，几乎没有任何一项是独自胜出太多的。所有这些都惊人地平等。

这一直是这种趋势。所以回答你的问题，我预计算法上会与今天发生的事情有所不同。但我也确实预计，一些长期存在的东西可能仍然会在那里。它可能仍然是一个用梯度下降训练的巨大神经网络。那会是我的猜测。

**采访者：**
令人惊讶的是，所有这些东西加在一起只把误差减半了，30 年的进步……也许一半很多了。因为如果你把误差减半，那实际上意味着……

**Andrej Karpathy：**
一半很多了。但我想让我震惊的是，一切都需要全面改进：架构、优化器、损失函数。而且它也一直在全面改进。所以我预计所有这些变化都将生机勃勃地继续下去。

## Nanochat 与 AI 辅助编程的现状

**采访者：** 是的。我正想问你一个关于 nanochat 的非常类似的问题。既然你最近才写完它的代码，构建聊天机器人的每一个步骤在你的“内存”里应该都还很新鲜。我很好奇你是否有类似的想法，比如：

**采访者：** “噢，从 GPT-2 到 nanochat 的过程中，并没有哪一件特定的事情是绝对关键的。” 在构建 nanochat 的经历中，有哪些让你意想不到的收获？nanochat 是我发布的一个代码库。

**Andrej Karpathy：** 是昨天还是前天发布的？我不记得了。我们可以看到为了做这个……我都缺觉成什么样了。

**Andrej Karpathy：** 它的目标是成为一个最简单、完整的代码库，涵盖构建 ChatGPT 克隆版的整个端到端流程。所以你拥有所有的步骤，而不仅仅是任何单独的一步，这一点很重要。过去我曾研究过所有的独立步骤，并发布过一些小段代码，从算法意义上用简单的代码展示那是如何完成的。但这一个处理的是整个流水线。

**Andrej Karpathy：** 就学习而言，我不确定我是否真的从中学到了什么新东西。我脑子里已经知道该怎么构建它了。这只是一个机械式构建的过程，并将其整理得足够干净，以便人们可以从中学习并觉得有用。别人从中学到的最好方法是什么？是把所有代码删掉，尝试从头重新实现，还是尝试在上面添加修改？

**Andrej Karpathy：** 这是一个很好的问题。这大概是 8,000 行代码，带你走完整个流程。我可能会把它放在右边的显示器上。如果你有两个显示器，你把它放在右边。如果你想从头开始构建，那就从头开始写。你不允许复制粘贴，你可以参考，但不允许复制粘贴。也许这就是我会采用的方法。

**Andrej Karpathy：** 但我也认为这个代码库本身是一个相当庞大的“怪兽”。当你写这段代码时，你不是从上到下写的，而是一块一块写的，并且你会扩展这些块，而这部分信息在代码库里是缺失的。你可能不知道从哪里开始。

**Andrej Karpathy：** 所以需要的不仅仅是最终的代码库，还有构建这个代码库的过程，这是一个复杂的“块增长”过程。那部分目前还没有。我很想在本周晚些时候加上这部分内容。可能是一个视频或类似的东西。

**Andrej Karpathy：** 粗略地说，这就是我会尝试做的事情。自己构建这些东西，但不允许自己复制粘贴。我确实认为有两种类型的知识。一种是高层次的表面知识，但是当你从头开始构建某样东西时，你会被迫去面对你不理解的东西，以及那些你甚至不知道自己不理解的东西。这总是会带来更深层次的理解。

**Andrej Karpathy：** 这是唯一的构建方式。如果我不能构建它，我就不理解它。我相信这是费曼的名言。我一直 100% 坚信这一点，因为有所有这些微小的细节没有被正确排列，你并不是真的拥有那些知识。你只是以为自己拥有了。

**Andrej Karpathy：** 所以不要写博客文章，不要做幻灯片，不要做那些事。编写代码，整理它，让它运行起来。这是唯一的出路。否则，你就缺失了知识。

**采访者：** 你发推文说，在这个代码库的组装过程中，编程模型对你的帮助非常小。我很好奇那是为什么。

**Andrej Karpathy：** 我大概花了一个多月的时间构建这个代码库。我想说，目前人们与代码交互的方式主要有三类。有一些人完全拒绝所有的 LLM（大型语言模型），他们只是从零开始手写。这可能不再是正确的做法了。

**Andrej Karpathy：** 中间部分，也就是我所处的位置，是你仍然从头开始写很多东西，但你会使用现在这些模型提供的自动补全功能。所以当你开始写一小段代码时，它会自动为你补全，你只需要按 Tab 键通过。大多数时候它是正确的，有时它不是，你就修改它。但你仍然是你所写内容的架构师。

**Andrej Karpathy：** 然后还有“氛围编程”（vibe coding）：即输入“嗨，请实现这个或那个”，回车，然后让模型去做。这就是智能体（Agents）。我确实觉得智能体在非常特定的场景下有效，我也会在特定场景下使用它们。

**Andrej Karpathy：** 但这些都是你可以使用的工具，你必须了解它们擅长什么，不擅长什么，以及何时使用它们。例如，如果你在做样板代码（boilerplate stuff），智能体就相当不错。对于那些只是复制粘贴的样板代码，它们非常擅长。它们非常擅长那些在互联网上经常出现的东西，因为这些模型的训练集中有很多这样的例子。

**Andrej Karpathy：** 对于具有某些特征的事物，模型会做得很好。我想说 nanochat 不是那类例子，因为它是一个相当独特的代码库。按照我的结构方式，并没有那么多现成的代码。它不是样板代码。

**Andrej Karpathy：** 它几乎是智力密集型的代码，一切都必须非常精确地排列。模型有太多的认知缺陷。举个例子，它们一直误解代码，因为它们对互联网上所有典型的做事方式有太多的记忆，而我并没有采用那些方式。例如，模型——我不知道是否要深入细节——但它们一直以为我在写普通的代码，但我不是。

**采访者：** 也许举一个例子？

**Andrej Karpathy：** 你有 8 个 GPU 都在进行前向和后向传播。在它们之间同步梯度的标准方法是使用 PyTorch 的分布式数据并行（Distributed Data Parallel, DDP）容器，它会在你进行后向传播时自动开始通信和同步梯度。

**Andrej Karpathy：** 我没有使用 DDP，因为我不想用，因为它没必要。我把它扔掉了，并在优化器的步骤中编写了我自己的同步例程。

**Andrej Karpathy：** 模型一直试图让我使用 DDP 容器。它们非常担心。这变得太技术化了，但我没有使用那个容器是因为我不需要它，而且我有类似功能的自定义实现。它们就是无法内化你有自己的一套东西这一点。

**Andrej Karpathy：** 它们无法跨过那个坎。它们一直试图搞乱风格。它们太过于防御性了。它们搞出所有这些 try-catch 语句。它们一直试图把它做成一个生产级的代码库，而在我的代码中有一堆假设，这没关系。

**Andrej Karpathy：** 我不需要里面有所有这些额外的东西。所以我感觉它们在让代码库变得臃肿，增加复杂性，它们不断误解，还多次使用已弃用的 API。简直是一团糟。它净值上并没用。我可以进去清理它，但这并不是净有用的。

**Andrej Karpathy：** 我也觉得必须要用英语打出我想要的东西很烦人，因为打字太多了。如果我只是导航到我想要的代码部分，去到我知道代码必须出现的地方，开始打出前几个字母，自动补全就会理解并直接给出代码。

**Andrej Karpathy：** 这是一种指定你想要什么的高信息带宽方式。你指向你想要代码的地方，打出前几个片段，模型就会补全它。所以我的意思是，这些模型在技术栈的某些部分表现良好。我有两个使用模型的例子我觉得很有说明意义。

**Andrej Karpathy：** 一个是我生成报告的时候。那更多是样板式的，所以我部分地“氛围编程”了一些内容。那没问题，因为它不是任务关键型的东西，而且运行良好。另一部分是我用 Rust 重写分词器（tokenizer）的时候。

**Andrej Karpathy：** 我不太擅长 Rust，因为我对 Rust 还很陌生。所以当我写一些 Rust 代码时，进行了一些氛围编程。但我有一个我已经完全理解的 Python 实现，我只是在确保我正在制作一个更高效的版本，而且我有测试，所以我做这些事情感觉更安全。

**Andrej Karpathy：** 它们增加了对你可能不太熟悉的语言或范式的可访问性。我认为它们在那方面也非常有帮助。外面有大量的 Rust 代码，模型对此相当擅长。我碰巧对此了解不多，所以模型在那里非常有用。

**采访者：** 这个问题之所以如此有趣，是因为人们关于 AI 爆炸式增长并迅速达到超级智能的主要故事，就是 AI 自动化 AI 工程和 AI 研究。

**采访者：** 他们会看到你可以拥有 Claude Code 并从头开始制作整个应用程序，比如 CRUD 应用程序，然后想，“如果你在 OpenAI 和 DeepMind 等公司内部拥有同样的能力，想象一下有一千个或一百万个并行的你，在寻找微小的架构调整。”

**采访者：** 听到你说这是它们表现得不对称地差的地方，这很有趣。这对于预测 AI 2027 年类型的爆炸是否可能很快发生非常相关。

**Andrej Karpathy：** 这是一个很好的说法，你也触及了为什么我的时间线稍微长一点的原因。你是对的。它们不太擅长从未被写过的代码，也许这是一种说法，而这正是我们在构建这些模型时试图实现的目标。

**采访者：** 问一个非常天真的问题，你在 nanochat 中添加的架构调整，它们在某处的论文里有，对吧？它们甚至可能在某处的代码库里。难道令人惊讶的是，当你像“添加 RoPE 嵌入”或类似的东西时，它们无法将其整合进去，或者它们做错了方式？

**Andrej Karpathy：** 这很难。它们知道，但它们并不完全知道。它们不知道如何将其完全整合到代码库、你的风格、你的位置以及你正在做的一些自定义事情中，以及它如何与代码库的所有假设相吻合。

**Andrej Karpathy：** 它们确实有一些知识，但它们还没有达到可以将其整合并使其合理的地步。很多东西都在持续改进。目前，我使用的最先进模型是 GPT-5 Pro，那是一个非常强大的模型。

**Andrej Karpathy：** 如果我有 20 分钟，我会复制粘贴我的整个代码库，然后去问 GPT-5 Pro 这个“先知”一些问题。通常它还不错，而且与一年前相比好得出奇。

**Andrej Karpathy：** 总的来说，模型还没有到位。我觉得这个行业正在做出太大的跳跃，试图假装这很神奇，但其实不是。它是“泔水”（slop）。他们没有正视这一点，也许他们试图融资或类似的事情。我不确定发生了什么，但我们正处于这个中间阶段。模型很神奇。它们仍然需要大量的工作。目前，自动补全是我的最佳平衡点。但有时，对于某些类型的代码，我会去求助于 LLM 智能体。

## AI 作为计算的延伸

**采访者：** 这是另一个非常有趣的原因。在编程的历史中，有过许多生产力改进——编译器、代码检查（linting）、更好的编程语言——这些都提高了程序员的生产力，但并没有导致爆炸。这听起来非常像自动补全标签页，而另一个类别仅仅是程序员的自动化。有趣的是，你看到的更多是属于更好编译器之类的历史类比类别。

**Andrej Karpathy：** 也许这引出了另一个想法。我很难区分 AI 的起点和终点，因为我看待 AI 从根本上来说是计算的一种延伸，而且是非常根本的方式。我看到这种递归自我改进或加速程序员的过程是一个连续体，一直追溯到开始：代码编辑器、语法高亮，甚至类型检查，比如数据类型检查——所有这些我们为彼此构建的工具。

**Andrej Karpathy：** 甚至搜索引擎。为什么搜索引擎不属于 AI？排名就是 AI。在某种程度上，谷歌，甚至在早期，就把自己看作是一家做谷歌搜索引擎的 AI 公司，这完全合理。我看待它比其他人更像是一个连续体，这让我很难划清界限。

**Andrej Karpathy：** 我觉得我们现在得到了一个好得多的自动补全，现在我们也得到了一些智能体，它们是那种循环的东西，但它们有时会脱轨。正在发生的事情是，人类正在逐渐做越来越少的低级工作。我们不写汇编代码，因为我们有编译器。

## 强化学习的局限性与过程监督

**Andrej Karpathy：** 编译器会把我的高级 C 语言代码写成汇编代码。我们在非常非常缓慢地抽象我们自己。这就是我所说的“自主性滑块”，越来越多的东西被自动化——在任何时间点可以被自动化的东西——我们做得越来越少，并在自动化之上的抽象层中提升自己。

**采访者：** 让我们谈谈强化学习（RL）。你在推特上发表了一些关于这个的非常有趣的内容。从概念上讲，我们应该如何思考人类能够仅通过与环境互动就构建丰富世界模型的方式，而且这种方式似乎几乎与这一集结尾的最终奖励无关？

**采访者：** 如果有人正在创业，在 10 年结束时，她发现生意是成功还是失败了，我们会说她获得了一堆智慧和经验。但这并不是因为过去 10 年发生的每一件事的对数概率（log probs）都被加权或减权了。某种更加深思熟虑和丰富的事情正在发生。机器学习（ML）的类比是什么，它与我们现在用 LLM 做的事情相比如何？

**Andrej Karpathy：** 也许我会这样说，人类不使用强化学习，正如我所说。我认为他们做的是不同的事情。强化学习比我认为普通人想象的要糟糕得多。强化学习很糟糕。只是恰好我们之前拥有的所有东西都更糟糕，因为以前我们只是在模仿人类，所以它有所有这些问题。

**Andrej Karpathy：** 在强化学习中，假设你在解决一个数学问题，因为它很简单。给你一个数学问题，你试图找到解决方案。在强化学习中，你会首先并行尝试很多事情。

**Andrej Karpathy：** 给你一个问题，你尝试数百种不同的尝试。这些尝试可能很复杂。它们可能是像，“噢，让我试试这个，让我试试那个，这个不行，那个不行，”等等。然后也许你得到了一个答案。

**Andrej Karpathy：** 现在你查看书的背面，你看到，“好的，正确答案是这个。”你可以看到这个、这个和那个得到了正确答案，但其他 97 个没有。强化学习字面上做的就是去到那些效果很好的尝试，把你沿途做的每一件事，每一个 token 都加权，就像“多做这个”。

**Andrej Karpathy：** 问题在于，人们会说你的估计器方差很高，但其实它只是嘈杂。它是嘈杂的。它几乎假设你为得出正确答案所做的每一个小步骤都是正确的事情，这并不真实。你可能走错了巷子，直到你得出了正确的解决方案。

**Andrej Karpathy：** 只要你得到了正确的解决方案，你所做的每一个不正确的事情都会被加权为“多做这个”。这很糟糕。这是噪音。你做了所有这些工作，结果只发现，最后你得到了一个像“噢，你做对了”的单一数字。

**Andrej Karpathy：** 基于此，你对整个轨迹进行加权，比如加权或减权。我喜欢的说法是你正在通过吸管吸取监督信号。你做了所有这些工作，可能是一分钟的展开（rollout），而你正在通过吸管吸取最终奖励信号的一点点监督位，并将其广播到整个轨迹上，用它来加权或减权该轨迹。这简直愚蠢且疯狂。人类永远不会这样做。

**Andrej Karpathy：** 第一，人类永远不会做数百次展开。第二，当一个人找到解决方案时，他们会有一个相当复杂的复盘过程，比如，“好的，我认为这部分我做得很好，这部分我做得不太好。我可能应该做这个或那个。”他们会思考。

**Andrej Karpathy：** 目前的 LLM 中没有任何东西能做到这一点。没有与之相当的东西。但我确实看到有论文冒出来试图做这件事，因为这对该领域的每个人来说都是显而易见的。

**Andrej Karpathy：** 顺便说一句，最初的模仿学习极其令人惊讶、神奇和了不起，我们可以通过模仿人类进行微调。那太不可思议了。因为在一开始，我们只有基础模型。基础模型就是自动补全。当时对我来说并不明显，我必须学习这一点。

**Andrej Karpathy：** 让我大吃一惊的论文是 InstructGPT，因为它指出你可以拿预训练模型，也就是自动补全，如果你只是在看起来像对话的文本上微调它，模型会非常迅速地适应变得非常会话化，并且它保留了预训练中的所有知识。

**Andrej Karpathy：** 这让我大吃一惊，因为我不明白在风格上，它可以调整得如此之快，仅通过几轮这类数据的微调就能成为用户的助手。对我来说，这非常神奇。太不可思议了。那是两到三年的工作。

**Andrej Karpathy：** 现在来了 RL。RL 允许你做得比仅仅模仿学习好一点，因为你可以拥有这些奖励函数，你可以根据奖励函数进行爬山算法。有些问题只有正确答案，你可以对此进行爬山，而无需获取专家轨迹来模仿。所以这很棒。模型还可以发现人类可能永远想不出的解决方案。这太不可思议了。

**Andrej Karpathy：** 然而，它仍然很愚蠢。我们需要更多。昨天我看到谷歌的一篇论文，试图在脑海中拥有这种反思和复盘的想法。

**采访者：** 是那个记忆库论文还是什么？

**Andrej Karpathy：** 我不知道。我见过几篇这方面的论文。所以我预计在这个领域，我们将对 LLM 的算法进行重大更新。我想我们需要再来三四个或五个这样的更新，差不多这样。

**采访者：** 你真的很擅长想出令人回味的短语。“通过吸管吸取监督信号。”太棒了。你是说基于结果的奖励的问题在于，你有这个巨大的轨迹，然后在最后，你试图从那最后的一点点信息中学习关于你应该做什么以及你应该了解世界的每一件可能的事情。

**采访者：** 既然这是显而易见的，为什么基于过程的监督作为一个替代方案还没有成为使模型更有能力的成功方法？是什么阻碍了我们使用这种替代范式？

**Andrej Karpathy：** 基于过程的监督只是指的是我们不会只在最后有一个奖励函数。在你做了 10 分钟的工作后，我不会告诉你你做得好还是不好。我要在你每一步的过程中告诉你你做得有多好。

**Andrej Karpathy：** 我们没有这个的原因是很难正确地做到这一点。你有部分解决方案，你不知道如何分配信用。所以当你得到正确答案时，这只是与答案的等式匹配。这很容易实现。如果你在做过程监督，你如何以自动化的方式分配部分信用？

**Andrej Karpathy：** 并不明显该怎么做。很多实验室试图用这些 LLM 评判者（LLM judges）来做。你让 LLM 试着去做。你提示一个 LLM，“嘿，看看学生的部分解决方案。如果答案是这个，你觉得他们做得怎么样？”他们试图调整提示。这之所以棘手，原因非常微妙。

**Andrej Karpathy：** 事实上，任何时候你使用 LLM 来分配奖励，那些 LLM 都是拥有数十亿参数的庞然大物，它们是可以被博弈的。如果你针对它们进行强化学习，你几乎肯定会找到你的 LLM 评判者的对抗性样本。

**Andrej Karpathy：** 所以你不能这样做太久。你可能做 10 步或 20 步，也许会有效，但你不能做 100 步或 1000 步。我明白这并不明显，但基本上模型会找到小裂缝。它会在巨大模型的角落和缝隙中找到所有这些虚假的东西，并找到欺骗它的方法。

**Andrej Karpathy：** 我脑海中有一个突出的例子，这可能是公开的，如果你使用 LLM 评判者作为奖励，你只给它一个学生的解决方案，问它学生做得好不好。我们针对那个奖励函数进行强化学习训练，效果非常好。然后，突然之间，奖励变得非常大。

**Andrej Karpathy：** 这是一个巨大的跳跃，它做得完美。你看着它就像，“哇，这意味着学生在所有这些问题上都很完美。它完全解决了数学。”但是当你查看从模型得到的补全时，它们完全是胡言乱语。

**Andrej Karpathy：** 它们开始还好，然后变成了“dhdhdhdh”。就像，“噢，好的，让我们算二加三，我们做这个和这个，然后 dhdhdhdh。”你看着它，就像，这太疯狂了。

**Andrej Karpathy：** 它是怎么得到 1 或 100% 的奖励的？你看一下 LLM 评判者，结果发现“dhdhdhdh”是该模型的对抗性样本，它给它分配了 100% 的概率。

**Andrej Karpathy：** 这仅仅是因为这是 LLM 的样本外示例。它在训练期间从未见过它，你处于纯粹的泛化领域。它在训练期间从未见过它，而在纯粹的泛化领域，你可以找到这些破坏它的例子。你基本上是在训练 LLM 成为一个提示注入模型。甚至不是那样。提示注入太花哨了。你正在寻找对抗性样本，正如它们所称的那样。

**Andrej Karpathy：** 这些是荒谬的解决方案，显然是错误的，但模型认为它们很棒。

**采访者：** 如果你认为这是使 RL 更具功能性的瓶颈，那么这将需要使 LLM 成为更好的评判者，如果你想以自动化的方式做到这一点。这是否只是一种类似 GAN 的方法，你必须训练模型使其更健壮？

**Andrej Karpathy：** 实验室可能正在做所有这些。显而易见的事情是，“dhdhdhdh”不应该得到 100% 的奖励。好的，那么，拿“dhdhdhdh”，把它放入 LLM 评判者的训练集中，并说这不是 100%，这是 0%。

**Andrej Karpathy：** 你可以这样做，但每次你这样做，你都会得到一个新的 LLM，它仍然有对抗性样本。有无穷多的对抗性样本。可能如果你迭代几次，可能会越来越难找到对抗性样本，但我不是 100% 确定，因为这东西有一万亿个参数或什么的。我敢打赌实验室正在尝试。但我仍然认为我们需要其他的想法。

**采访者：** 有趣。你对其他的想法可能是什么样子有什么构想吗？

**Andrej Karpathy：** 这个复盘解决方案的想法包含合成样本，这样当你对它们进行训练时，你会变得更好，并以某种方式元学习它。我想有一些论文我开始看到冒出来了。

**Andrej Karpathy：** 我只处于阅读摘要的阶段，因为很多这些论文只是想法。有人必须在前沿 LLM 实验室的规模上完全通用地使其工作，因为当你看到这些论文冒出来时，它只是有点嘈杂。它们是很酷的想法，但我还没看到任何人令人信服地表明这是可能的。

## 合成数据与模型坍缩

**Andrej Karpathy：** 话虽如此，LLM 实验室相当封闭，所以谁知道他们现在在做什么。

**采访者：** 我可以构想出你如何能够对自己制作的合成示例或合成问题进行训练。但这似乎还有人类做的另一件事——也许睡眠是这个，也许白日梦是这个——这不一定是想出假问题，而只是反思。

**采访者：** 我不确定白日梦或睡觉，或者仅仅是反思的 ML 类比是什么。我没有想出一个新问题。显然，最基本的类比就是在反思片段上进行微调，但在实践中我觉得那可能效果不会那么好。你对这件事的类比有什么看法吗？

**Andrej Karpathy：** 我确实认为我们在那里缺失了一些方面。举个例子，拿读书来说。目前当 LLM 读书时，这意味着我们拉长文本序列，模型预测下一个 token，它从中获得一些知识。

**Andrej Karpathy：** 那并不是人类真正做的事情。当你在读书时，我甚至不觉得书是我应该关注和训练的说明文。这本书是我进行合成数据生成的一组提示，或者是为了让你去读书俱乐部和你的朋友谈论它。

**Andrej Karpathy：** 正是通过操纵那些信息，你实际上获得了那些知识。我们目前在 LLM 中没有与之相当的东西。它们并不真正那样做。我很想看到在预训练期间有一些阶段，思考材料并试图将其与它已经知道的东西协调起来，并思考一段时间并使其工作。

**Andrej Karpathy：** 没有任何与之相当的东西。这都是研究。有一些微妙的——我认为很难理解的非常微妙的——原因解释了为什么这不是微不足道的。

**Andrej Karpathy：** 如果我可以描述其中一个：为什么我们不能只是合成生成并在其上训练？因为每一个合成示例，如果我只是给模型关于一本书的思考的合成生成，你看着它你会觉得，“这看起来很棒。为什么我不能在它上面训练？”你可以尝试，但如果你继续尝试，模型会变得更糟。

**Andrej Karpathy：** 那是因为你从模型得到的所有样本都在默默地坍缩。默默地——如果你看任何单个例子并不明显——它们占据了关于内容的可能思想空间的非常微小的流形。

**Andrej Karpathy：** LLM，当它们输出时，它们就是我们所说的“坍缩”了。它们有一个坍缩的数据分布。一个简单的查看方法是去 ChatGPT 问它，“给我讲个笑话。”它只有大概三个笑话。它没有给你所有可能笑话的广度。它知道大概三个笑话。它们默默地坍缩了。你没有从这些模型中得到像从人类那里得到的那样的丰富性、多样性和熵。

**Andrej Karpathy：** 人类要嘈杂得多，但至少他们在统计意义上没有偏见。他们没有默默地坍缩。他们保持着巨大的熵。

**Andrej Karpathy：** 那么，如何在尽管有坍缩的情况下让合成数据生成工作，同时保持熵呢？这是一个研究问题。

**采访者：** 只是为了确保我理解了，坍缩与合成数据生成相关的原因是因为你想要能够想出还没有在你的数据分布中的合成问题或反思？

**Andrej Karpathy：** 我想我想说的是，假设我们要读一本书的一章，我让一个 LLM 思考它，它会给你一些看起来非常合理的东西。但如果我问它 10 次，你会注意到它们都是一样的。

**Andrej Karpathy：** 你不能仅仅在相同数量的提示信息上不断扩展“反思”然后从中获得回报。任何单个样本看起来都还可以，但它的分布相当糟糕。它糟糕到如果你在太多自己的东西上继续训练，你实际上会坍缩。我认为对此可能没有根本的解决方案。

**Andrej Karpathy：** 我也认为人类随着时间的推移会坍缩。这些类比出奇地好。人类在他们的一生中会坍缩。这就是为什么孩子，他们还没有过拟合。

**Andrej Karpathy：** 他们会说出让你震惊的话，因为你可以看到他们的出发点，但这只是人们不会说的话，因为他们还没有坍缩。但我们坍缩了。我们最终重访相同的想法。我们最终说越来越多相同的话，学习率下降，坍缩继续恶化，然后一切都在恶化。

**采访者：** 你看过这篇超级有趣的论文吗，说做梦是一种防止这种过拟合和坍缩的方式？做梦之所以具有进化适应性，是为了把你置于非常不像你日常现实的奇怪情境中，以防止这种过拟合。

**Andrej Karpathy：** 这是一个有趣的想法。我确实认为当你在脑海中生成事物然后你关注它时，你是在你自己的样本上训练，你在你的合成数据上训练。如果你做得太久，你会脱轨并且你会过度坍缩。你总是必须在生活中寻求熵。

**Andrej Karpathy：** 与其他人交谈是熵的一个很好的来源，诸如此类的事情。所以也许大脑也建立了一些内部机制来在这个过程中增加熵的数量。那是一个有趣的想法。

**采访者：** 这是一个非常不成熟的想法，所以我只是把它抛出来让你反应。我们所知道的最好的学习者，也就是孩子，在回忆信息方面极差。

**采访者：** 事实上，在童年的最初阶段，你会忘记一切。你对某个日期之前发生的一切都是健忘的。

**采访者：** 但你非常擅长学习新语言和从世界中学习。也许有一些能够透过树木看到森林的元素。而如果你把它与光谱的另一端相比，你有 LLM 预训练，这些模型实际上能够逐字逐句地反刍维基百科页面中的下一件事。

**采访者：** 但它们像孩子一样快速学习抽象概念的能力要有限得多。然后成年人介于两者之间，他们没有童年学习的灵活性，但他们可以用一种孩子难以做到的方式记忆事实和信息。

**采访者：** 我不知道在这个光谱中是否有什么有趣的东西。

**Andrej Karpathy：** 我认为那里有一些非常有趣的东西，100%。我确实认为人类比起 LLM 有更多的元素是透过树木看森林。我们实际上并不那么擅长记忆，这实际上是一个特性。

**Andrej Karpathy：** 因为我们不那么擅长记忆，我们被迫在更一般的意义上寻找模式。相比之下，LLM 极其擅长记忆。它们会背诵所有这些训练来源的段落。你可以给它们完全荒谬的数据。

**Andrej Karpathy：** 你可以哈希一些文本或类似的东西，你会得到一个完全随机的序列。如果你在上面训练，即使只是一次迭代或两次，它也能突然反刍整个东西。它会记住它。一个人不可能读一个单一的随机数字序列并向你背诵它。

**Andrej Karpathy：** 这是一个特性，而不是一个错误，因为它迫使你只学习可概括的组件。而 LLM 被它们对预训练文档的所有记忆分散了注意力，这在某种意义上对它们来说可能非常令人分心。

**Andrej Karpathy：** 所以这就是为什么当我谈论认知核心时，我想移除记忆，这就是我们谈到的。我很想让它们拥有更少的记忆，以便它们必须查阅资料，它们只保留思考的算法、实验的想法以及所有这些行动的认知粘合剂。

**采访者：** 这也与防止模型坍缩有关吗？

**Andrej Karpathy：** 让我想想。我不确定。这几乎就像一个单独的轴。模型太擅长记忆了，不知何故我们应该移除它。人类差得多，但这是一件好事。

**Andrej Karpathy：** 模型坍缩的解决方案是什么？你可以尝试非常天真的事情。logits 上的分布应该更宽或什么的。你可以尝试很多天真的事情。

**采访者：** 天真方法的问题最终是什么？

**Andrej Karpathy：** 这是一个很好的问题。你可以想象对熵等进行正则化。我想它们在经验上效果不佳，因为现在的模型是坍缩的。但我会说，我们需要的大多数任务实际上并不需要多样性。这可能是正在发生的事情的答案。

**Andrej Karpathy：** 前沿实验室正试图让模型有用。我觉得输出的多样性并没有那么多……第一，它更难处理和评估以及所有这些东西，但也许这不是捕获大部分价值的地方。事实上，它是受到积极惩罚的。

**采访者：** 如果你在 RL 中超级有创意，那是不好的。

**Andrej Karpathy：** 是的。或者如果你在做大量的写作，从 LLM 获得帮助之类的，这可能很糟糕，因为模型会默默地给你所有相同的东西。它们不会探索很多不同的回答问题的方式。

**Andrej Karpathy：** 也许这种多样性，并没有那么多应用需要它，所以模型没有它。但在合成数据生成时，这就是一个问题，等等。所以我们不允许这种熵在模型中保持，是在搬起石头砸自己的脚。可能实验室应该更加努力。

**采访者：** 我想你暗示这是一个非常基本的问题，不容易解决。你对此的直觉是什么？

**Andrej Karpathy：** 我不知道它是否超级基本。我不知道我是否打算那样说。我确实认为我还没做过这些实验，但我确实认为你可能可以将熵正则化得更高。所以你鼓励模型给你越来越多的解决方案，但你不想让它开始过多地偏离训练数据。

**Andrej Karpathy：** 它会开始编造自己的语言。它会开始使用极其罕见的词，所以它会偏离分布太远。所以我认为控制分布只是很棘手。在这个意义上，这可能不是微不足道的。

## 认知核心的大小与模型规模

**采访者：** 如果你必须猜测，最佳的智能核心最终应该是多少位？我们放在冯·诺依曼探测器上的东西，它必须有多大？

**Andrej Karpathy：** 在这个领域的历史上这真的很有趣，因为曾几何时，一切都在规模上非常激进，比如，“噢，我们要制造更大的模型，万亿参数的模型。”模型在尺寸上所做的是它们上升了，现在它们下来了。最先进的模型更小了。即便如此，我认为它们还是记住了太多东西。

**Andrej Karpathy：** 所以我之前有一个预测，我几乎觉得我们可以得到即使在十亿参数下也非常好的认知核心。如果你和一个十亿参数的模型交谈，我认为在 20 年内，你可以进行非常有成效的对话。

**Andrej Karpathy：** 它会思考，而且更像人类。但如果你问它一些事实性问题，它可能必须查阅，但它知道它不知道，它可能必须查阅，它只会做所有合理的事情。

**采访者：** 很惊讶你认为这需要十亿参数。因为我们已经有了十亿参数模型或几十亿参数模型，它们非常智能。

**Andrej Karpathy：** 嗯，最先进的模型大概是一万亿参数。但它们记住了太多东西。

**采访者：** 是的，但我很惊讶在 10 年内，考虑到这个速度……我们有 gpt-oss-20b。那比 GPT-4 原版好得多，后者是一万亿以上的参数。

**采访者：** 考虑到这个趋势，我很惊讶你认为在 10 年内认知核心仍然是十亿参数。我很惊讶你不是说，“噢，它会是几千万或几百万。”

**Andrej Karpathy：** 问题在于，训练数据是互联网，这真的很糟糕。有巨大的收益空间，因为互联网很糟糕。甚至互联网，当你和我想象互联网时，你想的是像《华尔街日报》。那不是这个。当你在前沿实验室看预训练数据集并查看随机的互联网文档时，它是完全的垃圾。

**Andrej Karpathy：** 我甚至不知道这到底是怎么运作的。它是一些像股票代码、符号之类的，它是来自互联网各个角落的大量泔水和垃圾。它不像你的《华尔街日报》文章，那是极其罕见的。

**Andrej Karpathy：** 所以因为互联网太糟糕了，我们必须构建非常大的模型来压缩所有这些。大部分压缩是记忆工作而不是认知工作。但我们真正想要的是认知部分，删除记忆。

**Andrej Karpathy：** 我想我想说的是，我们需要智能模型来帮助我们甚至提炼预训练集，将其缩小到认知组件。然后我认为你可以使用更小的模型，因为它是一个更好的数据集，你可以在上面训练它。但可能它不是直接在上面训练的，它可能是从一个更好的模型中蒸馏出来的。

**采访者：** 但为什么蒸馏版本仍然是十亿？

**Andrej Karpathy：** 我只是觉得蒸馏效果极好。所以几乎每一个小模型，如果你有一个小模型，它几乎肯定是被蒸馏过的。

**采访者：** 对，但为什么 10 年内的蒸馏不能低于 10 亿？

**Andrej Karpathy：** 噢，你觉得它应该比十亿更小？我的意思是，拜托，对吧？我不知道。在某些时候，至少需要十亿个旋钮才能做一些有趣的事情。

**采访者：** 你认为它应该更小？

**Andrej Karpathy：** 是的。如果你看看过去几年的趋势，仅仅是寻找唾手可得的果实，从万亿以上的模型到在短短两年内实际上小了两个数量级并且性能更好的模型，这让我觉得智能的核心可能会更小得多。

**Andrej Karpathy：** 底部有足够的空间，套用费曼的话。我觉得谈论十亿参数的认知核心我已经很反向操作了，而你比我还激进。也许我们可以变得更小一点。

**Andrej Karpathy：** 我确实认为实际上，你希望模型拥有一些知识。你不想让它查阅所有东西，因为那样你就无法在脑子里思考。你一直查阅太多东西了。一些基本的课程知识需要在那里，但它没有深奥的知识。

**采访者：** 我们正在讨论认知核心可能是什么。还有一个单独的问题，那就是随着时间的推移，前沿模型的规模将会是多少？

**采访者：** 我很好奇你是否有预测。我们的规模一直在增加，直到大概 GPT 4.5，现在我们看到规模在减少或停滞。这可能有很多原因。你对未来有预测吗？最大的模型会更大，会更小，还是会一样？

**Andrej Karpathy：** 我没有超强的预测。实验室只是很务实。他们有浮点运算（flops）预算和成本预算。结果表明，预训练不是你想投入大部分浮点运算或成本的地方。

**Andrej Karpathy：** 这就是为什么模型变得更小了。它们稍微小了一点，预训练阶段更小，但它们在强化学习、中期训练以及随后的所有步骤中弥补了这一点。他们只是在就所有阶段以及如何获得最大性价比方面务实。预测这一趋势相当困难。

**Andrej Karpathy：** 我仍然预计有太多唾手可得的果实。这是我的基本预期。我有非常广泛的分布。

**采访者：** 你预计这些唾手可得的果实与过去两到五年发生的事情在种类上相似吗？如果你看 nanochat 与 nanoGPT 以及你所做的架构调整，那是你预期会继续发生的事情类型吗？

**Andrej Karpathy：** 你不期望任何巨大的范式转变。

**Andrej Karpathy：** 在很大程度上，是的。我预计数据集会变得好得多。当你查看平均数据集时，它们极其糟糕。它们太糟糕了，以至于我甚至不知道任何东西是如何运作的。看看训练集中的平均例子：事实性错误、谬误、荒谬的事情。

**Andrej Karpathy：** 不知何故，当你大规模做这件事时，噪音被冲走了，你剩下了一些信号。数据集将会有巨大的改进。一切都会变得更好。我们的硬件，运行硬件的所有内核，以及最大化利用硬件。

**Andrej Karpathy：** 英伟达正在慢慢调整硬件本身，Tensor Cores，所有这些都需要发生并将继续发生。所有的内核都会变得更好，并在最大程度上利用芯片。所有的算法可能会在优化、架构以及所有建模组件以及我们甚至用来训练的算法方面得到改进。

**Andrej Karpathy：** 我确实预计没有哪一项占主导地位。每样东西加 20%。这大概就是我所看到的。

## AGI 的定义与经济影响

**采访者：** 人们提出了不同的方法来绘制我们在通往完全 AGI（通用人工智能）方面取得了多少进展。如果你能想出某条线，那么你就可以看到那条线在哪里与 AGI 相交，以及那会在 x 轴的哪里发生。

**采访者：** 人们提出这是教育水平。我们有一个高中生，然后他们通过 RL 上了大学，他们将获得博士学位。我不喜欢那个。或者他们会提出视野长度。也许他们可以做需要一分钟的任务，他们可以自主完成。然后他们可以自主完成需要人类一小时、人类一周的任务。

**采访者：** 你如何看待这里相关的 y 轴？我们应该如何思考 AI 是如何取得进步的？

**Andrej Karpathy：** 我对此有两个答案。第一，我几乎想完全拒绝这个问题，因为我看待这是计算的一种延伸。我们谈论过如何绘制计算的进步吗，或者自 1970 年代以来你如何绘制计算的进步？

**采访者：** y 轴是什么？

**Andrej Karpathy：** 从那个角度来看，整个问题有点好笑。

**Andrej Karpathy：** 当人们谈论 AI 和最初的 AGI 以及当 OpenAI 开始时我们如何谈论它时，AGI 是一个你可以求助的系统，它可以以人类表现或更好的水平完成任何有经济价值的任务。那是定义。当时我对那个定义很满意。

**Andrej Karpathy：** 我一直坚持那个定义，然后人们编造了各种各样的其他定义。但我喜欢那个定义。人们经常做出的第一个让步是，他们只是拿走了所有的物理东西，因为我们只谈论数字知识工作。

**Andrej Karpathy：** 与最初的定义相比，这是一个相当重大的让步，最初的定义是人类可以做的任何任务。我可以举起东西，等等。AI 显然做不到那个，但我们接受这个。

**Andrej Karpathy：** 通过说“噢，只有知识工作”，我们拿走了经济的多少比例？我不知道数字。如果我要猜的话，我觉得大约 10% 到 20% 只是知识工作，某人可以在家工作并执行任务，大概是这样。它仍然是一个非常大的市场。

**Andrej Karpathy：** 经济的规模是多少，10% 或 20% 是多少？即使在美国，我们仍然在谈论几万亿美元的市场份额或工作。所以它仍然是一个非常巨大的桶。

**Andrej Karpathy：** 回到定义，我会寻找的是那个定义在多大程度上是真实的？有没有工作或很多任务？如果我们把任务看作不是工作而是任务。

**Andrej Karpathy：** 这很困难，因为问题在于社会会根据构成工作的任务，根据什么是可自动化的或不可自动化的来进行重构。今天，什么工作可以被 AI 取代？

**Andrej Karpathy：** 最近的一个好例子是杰夫·辛顿（Geoff Hinton）预测放射科医生将不再是一份工作，结果证明这在很多方面都大错特错。放射科医生依然存在并且还在增长，即使计算机视觉在识别他们必须在图像中识别的所有不同事物方面真的非常非常好。

**Andrej Karpathy：** 这只是一份混乱、复杂的工作，有很多接触面，还要处理病人以及这背景下的所有事情。我不知道按照那个定义 AI 是否已经产生了巨大的影响。

**Andrej Karpathy：** 我会寻找的一些工作具有一些特征，使其比其他工作更早地适合自动化。举个例子，呼叫中心员工经常被提起，我认为这是正确的。

**Andrej Karpathy：** 就今天什么是可自动化的而言，呼叫中心员工具有许多简化属性。他们的工作相当简单。这是一系列任务，每个任务看起来都很相似。你接听一个人的电话，这是 10 分钟的互动或不管多久，可能稍微长一点。

**采访者：** 根据我的经验，要长得多。

**Andrej Karpathy：** 你在某种方案中完成一些任务，你更改一些数据库条目或类似的东西。所以你一直在重复某件事，这就是你的工作。你确实想引入任务视野——执行任务需要多长时间——然后你也想移除上下文。

**Andrej Karpathy：** 你不需要处理公司服务的不同部分或其他客户。这只是数据库、你和你正在服务的某个人。它是更封闭的，它是更可理解的，它是纯数字的。

**Andrej Karpathy：** 所以我会寻找那些东西。但即使在那里，我也还没有看到完全自动化。我在寻找自主性滑块。我预计我们不会立即取代人。我们将换入能做 80% 量的 AI。

**Andrej Karpathy：** 它们将 20% 的量委托给人类，人类正在监督五人 AI 团队做更死板的呼叫中心工作。我会寻找新的界面或新的公司，提供某种层，允许你管理其中一些尚不完美的 AI。

**Andrej Karpathy：** 然后我会预计这会在整个经济中发生。很多工作比呼叫中心员工要难得多。

**采访者：** 关于放射科医生，我完全是在推测，我不知道放射科医生的实际工作流程包括什么。但一个可能适用的类比是，当 Waymo 最初推出时，前排座位上会坐一个人，你只需要让他们在那里，以确保如果出了大问题，他们在那里监控。即使在今天，人们仍在观察以确保事情顺利进行。

**采访者：** 刚刚部署的 Robotaxi 里面仍然有一个人。现在我们可能处于类似的情况，如果你自动化了 99% 的工作，人类必须做的那最后 1% 极其有价值，因为它成为了其他一切的瓶颈。

**采访者：** 如果放射科医生的情况是这样，坐在 Waymo 前面的人必须经过多年的专门训练才能提供那最后的 1%，他们的工资应该会大幅上涨，因为他们是广泛部署的唯一瓶颈。

**Andrej Karpathy：** 放射科医生，我认为他们的工资上涨也是出于类似的原因，如果你是最后的瓶颈并且你是不可替代的。Waymo 司机可能是可以被其他人替代的。

**采访者：** 所以你可能会看到这种事情，你的工资上涨直到你达到 99%，然后当最后 1% 消失时就像那样下跌。我想知道我们是否在放射科或呼叫中心工人的工资或类似的事情上看到了类似的情况。

**Andrej Karpathy：** 这是一个有趣的问题。我不认为我们在放射科看到了那个。我认为放射科不是一个好例子。我不知道为什么杰夫·辛顿挑放射科，因为我认为那是一个极其混乱、复杂的职业。

**Andrej Karpathy：** 我会对今天呼叫中心员工发生的事情更感兴趣，例如，因为我会预计很多死板的东西今天就可以自动化。我没有第一手的访问权限，但我会寻找呼叫中心员工发生了什么的趋势。

**Andrej Karpathy：** 我也会预期的一些事情是，也许他们正在换入 AI，但我仍然会等一两年，因为我可能会预期他们会撤回并重新雇用一些人。

![标准版信息图](/images/karpathy标准版信息图.avif)

## 为什么编程是 AI 的最佳应用场景

**Andrej Karpathy：** 已经有证据表明这在采用 AI 的公司中普遍发生，我认为这相当令人惊讶。我还发现真正令人惊讶的是。AGI，对吧？一个可以做一切的东西。我们将拿走体力劳动，但它应该能够做所有的知识工作。

**采访者：** 你天真地预期的进展方式是，你拿走顾问正在做的一个小任务，把它从桶里拿出来。你拿走会计正在做的一个小任务，把它从桶里拿出来。然后你只是在所有的知识工作中这样做。

**采访者：** 但相反，如果我们相信我们正处于当前范式的 AGI 道路上，进展非常不像那样。似乎顾问和会计并没有获得巨大的生产力提升。这非常像是程序员的工作被越来越多地削减。

**采访者：** 如果你看这些公司的收入，扣除正常的聊天收入——这类似于谷歌或什么的——只看 API 收入，它由编码主导。所以这个“通用”的东西，应该能够做任何知识工作，却绝大多数只在做编码。这是你会预期 AGI 被部署的一种令人惊讶的方式。

**Andrej Karpathy：** 这里有一个有趣的观点。我确实相信编码是这些 LLM 和智能体的完美首选。那是因为编码一直从根本上围绕文本工作。它是计算机终端和文本，一切都基于文本。

**Andrej Karpathy：** LLM，正如它们在互联网上被训练的方式，喜欢文本。它们是完美的文本处理器，外面有所有这些数据。这是一个完美的契合。我们也有很多为处理代码和文本预先构建的基础设施。

**Andrej Karpathy：** 例如，我们有 Visual Studio Code 或你最喜欢的 IDE 显示代码，智能体可以插入其中。如果智能体有一个 diff（差异），它做了一些更改，我们突然就有了所有这些现成的代码，使用 diff 显示代码库的所有差异。

**Andrej Karpathy：** 几乎就像我们已经为代码预先构建了很多基础设施。将其与一些完全不享受这些东西的事物进行对比。

**Andrej Karpathy：** 举个例子，有人试图构建自动化，不是为了编码，而是为了幻灯片。我看到一家公司做幻灯片。那要难得多，难得多。之所以难得多，是因为幻灯片不是文本。幻灯片是小图形，它们在空间上排列，有一个视觉组件。幻灯片没有这种预先构建的基础设施。

**Andrej Karpathy：** 例如，如果一个智能体要更改你的幻灯片，东西怎么向你显示 diff？你怎么看 diff？没有什么东西能显示幻灯片的 diff。有人必须构建它。正如它们现在的样子，即文本处理器，有些东西不适合 AI，而代码出奇地适合。

**采访者：** 我不确定这本身能解释它。我个人曾试图让 LLM 在纯语言输入、语言输出的领域变得有用，比如重写成绩单、根据成绩单想出剪辑。

**采访者：** 很可能我没有做每一件可能的事情。我在上下文中放了一堆好的例子，但也许我应该做某种微调。

**采访者：** 我们共同的朋友 Andy Matuschak 告诉我，他尝试了 500 亿种方法试图让模型擅长编写间隔重复提示。同样，这非常像是语言输入、语言输出的任务，这类事情应该是这些 LLM 全部技能的正中心。

**采访者：** 他尝试了带有一些少样本示例的上下文学习。他尝试了监督微调和检索。他无法让它们制作出令他满意的卡片。所以我发现这很惊人，即使在语言输出领域，除了编码之外，也很难从这些模型中获得很多经济价值。

**Andrej Karpathy：** 我不知道什么能解释它。这有道理。我不是说任何文本都是微不足道的。我确实认为代码是非常结构化的。文本可能更华丽，而且文本中有更多的熵，我会这么说。

## 超级智能、失控与多智能体竞争

**Andrej Karpathy：** 我不知道还能怎么说。而且代码很难，所以人们感到 LLM 赋予了他们很大的权力，即使是简单的知识。我不确定我有一个很好的答案。显然，文本让它变得容易得多，但这并不意味着所有的文本都是微不足道的。

**采访者：** 你如何看待超级智能？你预计它感觉上会与正常人类或人类公司有质的不同吗？

**Andrej Karpathy：** 我把它看作是社会自动化的一种进程。推断计算的趋势，会有很多东西的逐渐自动化，而超级智能将是那的一种推断。

**Andrej Karpathy：** 我们预计随着时间的推移会有越来越多的自主实体，它们正在做大量的数字工作，然后最终甚至在一段时间后做物理工作。基本上我看待它大概就是自动化。

**采访者：** 但自动化包括人类已经能做的事情，而超级智能意味着人类不能做的事情。

**Andrej Karpathy：** 但人们做的一件事就是发明新东西，如果这说得通的话，我只会把它放入自动化中。

**采访者：** 但我想，如果不那么抽象而更定性一点，你是否预计有些东西感觉像……因为这东西要么思考得太快，要么有太多的副本，或者副本可以合并回自身，或者更聪明，AI 可能拥有的任何数量的优势，这些 AI 存在的文明是否只会感觉与人类有质的不同？

**Andrej Karpathy：** 我认为会的。它从根本上是自动化，但它将是非常陌生的。它看起来会真的很奇怪。就像你提到的，我们可以在计算机集群上运行所有这些，而且快得多。

**Andrej Karpathy：** 当世界变成那样时，我开始感到紧张的一些场景是这种逐渐失去控制和对正在发生的事情的理解。我认为那是通过最可能的结果，即会有逐渐的理解丧失。

**Andrej Karpathy：** 我们会逐渐把所有这些东西铺设在各个地方，懂得它的人会越来越少。然后会对发生的事情逐渐失去控制和理解。这对我来说似乎是所有这些事情将如何发生的最可能结果。

**采访者：** 让我探讨一下。我不清楚失去控制和失去理解是否是一回事。

**采访者：** 台积电、英特尔的董事会——随便说一家公司——他们只是有名望的 80 岁老人。他们的理解很少，也许他们实际上并没有控制权。

**采访者：** 一个更好的例子是美国总统。总统拥有很大的权力。我不是想对当前的运作发表好声明，或者也许我是，但实际的理解水平与控制水平截然不同。

**Andrej Karpathy：** 我认为这很公平。这是一个很好的反驳。我想我预计两者都会失去。

**采访者：** 怎么会？失去理解是显而易见的，但为什么会失去控制？

**Andrej Karpathy：** 我们真的进入了一个我不知道这看起来像什么的领域，但如果我要写科幻小说，它们看起来会像不是一个单一的实体接管一切，而是多个竞争实体逐渐变得越来越自主。

**Andrej Karpathy：** 它们中的一些变坏了，其他的击退它们。这是我们委托给的完全自主活动的“火锅”。

**Andrej Karpathy：** 我觉得会有那种味道。并不是它们比我们聪明导致失去控制。而是它们彼此竞争，而从那种竞争中产生的任何东西都会导致失去控制。

## 智能爆炸还是线性增长？

**Andrej Karpathy：** 很多这些东西，它们将是人们的工具，它们代表人们行事或类似的事情。所以也许那些人在控制，但也许是对社会整体而言，就我们想要的结果而言，这是一种失控。你拥有代表个人行事的实体，这些实体仍然大致被视为失控的。

**采访者：** 这是一个我应该早点问的问题。我们谈到了目前感觉当你做 AI 工程或 AI 研究时，这些模型更属于编译器类别而不是替代品类别。在某种程度上，如果你有 AGI，它应该能够做你做的事。

**采访者：** 你觉得拥有并行的一百万个你会导致 AI 进步的巨大加速吗？如果那真的发生了，你预计一旦我们有了真正的 AGI，会看到智能爆炸吗？

**Andrej Karpathy：** 我不是在谈论今天的 LLM。我确实这么认为，但这只是照常营业，因为我们已经处于智能爆炸中，并且已经持续了几十年。这基本上是 GDP 曲线，它是行业这么多方面的指数加权总和。

**Andrej Karpathy：** 一切都在逐渐自动化，并且已经持续了数百年。工业革命是自动化以及一些物理组件和工具构建等所有这些东西。编译器是早期的软件自动化，等等。我们已经递归地自我改进和爆炸很长时间了。

**Andrej Karpathy：** 另一种看法是，如果你不看生物力学等，地球是一个相当无聊的地方，看起来非常相似。如果你从太空看，我们正处于这个鞭炮事件的中间，但我们在慢动作中看到它。我绝对觉得这已经发生了很长时间。

**采访者：** 同样，我不认为 AI 是与已经发生很长时间的事情截然不同的技术。你认为它是这种超指数趋势的延续？

**Andrej Karpathy：** 是的。这就是为什么这对我来说非常有趣，因为我有一段时间试图在 GDP 中找到 AI。我认为 GDP 应该上升。但后来我看了一些我认为非常具有变革性的其他技术，比如计算机或手机等等。

**Andrej Karpathy：** 你在 GDP 中找不到它们。GDP 是相同的指数。即使早期的 iPhone 也没有 App Store，它没有现代 iPhone 拥有的很多花里胡哨的东西。所以即使我们认为 2008 年 iPhone 问世是这种重大的地震式变化，实际上并非如此。

**Andrej Karpathy：** 一切都如此分散，它扩散得如此缓慢，以至于一切最终都被平均到相同的指数中。计算机也是完全一样的事情。你在 GDP 中找不到它们，就像，“噢，我们现在有电脑了。”那不是发生的事情，因为它是如此缓慢的进展。

**Andrej Karpathy：** 对于 AI，我们将看到完全相同的事情。这只是更多的自动化。它允许我们编写以前无法编写的不同类型的程序，但 AI 从根本上仍然是一个程序。

**Andrej Karpathy：** 它是一种新型计算机和新型计算系统。但它有所有这些问题，它会随着时间的推移扩散，它仍然会加起来成为相同的指数。我们仍然会有一个变得极其垂直的指数。生活在那样的环境中将会非常陌生。

**采访者：** 你是说，如果你看工业革命之前到现在的趋势，你有一个超指数，你从 0% 的增长到 10,000 年前的 0.02% 增长，再到现在的 2% 增长。这是一个超指数。你是说如果你把 AI 绘制在那里，那么 AI 会带你到 20% 的增长或 200% 的增长？

**采访者：** 还是你是说如果你看过去 300 年，你所看到的是你有一个又一个的技术——计算机、电气化、蒸汽机、铁路等等——但增长率完全相同，它是 2%。你是说增长率会上升吗？

**Andrej Karpathy：** 增长率也大致保持不变，对吧？仅在过去 200、300 年里。但在人类历史的过程中，它爆炸了。它从 0% 变得越来越快，越来越快，越来越快。工业爆炸，2%。

**Andrej Karpathy：** 有一段时间我试图找到 AI 或在 GDP 曲线中寻找 AI，我已经说服自己这是错误的。即使当人们谈论递归自我改进和实验室等等时，这也是照常营业。

**Andrej Karpathy：** 当然它会递归地自我改进，而且它一直在递归地自我改进。LLM 允许工程师更高效地工作以构建下一轮 LLM，更多的组件正在被自动化和调整等等。所有的工程师都能使用谷歌搜索也是其中的一部分。

**Andrej Karpathy：** 所有的工程师都有 IDE，所有人都有自动补全或拥有 Claude code 等等，这都只是整个事情相同加速的一部分。它只是如此平滑。

**采访者：** 只是为了澄清，你是说增长率不会改变。智能爆炸将表现为它只是让我们能够继续保持在 2% 的增长轨迹上，就像互联网帮助我们保持在 2% 的增长轨迹上一样。

**Andrej Karpathy：** 是的，我的预期是它保持在相同的模式中。

**采访者：** 只是为了向你提出相反的论点，我的预期是它会爆炸，因为我认为真正的 AGI——我不是在谈论 LLM 编码机器人，我是在谈论在服务器中实际替代人类——与这些其他提高生产力的技术有质的不同，因为它本身就是劳动力。我认为我们生活在一个劳动力非常受限的世界里。

**采访者：** 如果你和任何创业公司创始人或任何人交谈，你可以像，你需要更多什么？你需要真正有才华的人。如果你有数十亿额外的人在发明东西，整合自己，从头到尾建立公司，这感觉与单一技术有质的不同。这就像你在地球上获得了 100 亿额外的人。

**Andrej Karpathy：** 也许是一个反驳点。我很愿意在这个点上被说服。但我会说，例如，计算就是劳动力。计算曾经是劳动力。计算机，很多工作消失了，因为计算机正在自动化一堆数字信息处理，你现在不需要人类来做。

**Andrej Karpathy：** 所以计算机是劳动力，这已经发生了。举个例子，自动驾驶也是计算机在做劳动力。这也已经在发生了。这仍然是照常营业。你有机器正在以可能更快的速度吐出更多像那样的东西。

**采访者：** 从历史上看，我们有增长机制改变的例子，你从 0.2% 的增长变成了 2% 的增长。这对我来说似乎很合理，一台正在吐出下一个自动驾驶汽车和下一个互联网以及不管什么的机器……

**Andrej Karpathy：** 我明白这来自哪里。与此同时，我确实觉得人们做出了这个假设，“我们在盒子里有上帝，现在它可以做一切，”而它只是不会看起来像那样。它将能够做一些事情。

**Andrej Karpathy：** 它会在其他一些事情上失败。它将被逐渐投入社会，我们将最终得到相同的模式。这是我的预测。这种突然拥有一个完全智能、完全灵活、完全通用的人类在盒子里，我们可以把它分配给社会中的任意问题，我不认为我们会发生这种离散的变化。

**Andrej Karpathy：** 我认为我们将达到这种在整个行业中逐渐扩散的相同类型。在这些对话中，这往往最终具有误导性。

**采访者：** 我不喜欢在这种背景下使用“智能”这个词，因为智能意味着你认为会有一个单一的超级智能坐在服务器里，它会预测如何想出导致这种爆炸的新技术和发明。那不是我在想象 20% 增长时所想象的。

**采访者：** 我想象有数十亿非常聪明的人类般的头脑，可能，或者这就是所需要的全部。但事实上有数亿个，数十亿个，每一个都在单独制造新产品，弄清楚如何将自己整合到经济中。

**采访者：** 如果一个经验非常丰富的聪明移民来到这个国家，你不需要弄清楚我们如何将他们整合到经济中。他们自己弄清楚。他们可以创办公司，他们可以发明创造，或者提高世界的生产力。

**采访者：** 我们有例子，即使在当前的机制下，也有地方实现了 10-20% 的经济增长。如果你只是有很多人，与人相比资本较少，你可以拥有香港或深圳或不管哪里，拥有几十年的 10% 以上的增长。

**采访者：** 有很多非常聪明的人准备利用资源并做这段追赶期，因为我们有了这种不连续性，我认为 AI 可能会类似。

**Andrej Karpathy：** 我明白，但我仍然认为你在预设某种离散的跳跃。有一些我们等待认领的解锁。突然间我们将拥有数据中心里的天才。我仍然认为你在预设某种没有历史先例的离散跳跃，我在任何统计数据中都找不到，而且我认为可能不会发生。

**采访者：** 我的意思是，工业革命就是这样一个跳跃。你从 0.2% 的增长到了 2% 的增长。我只是说你会看到另一个像那样的跳跃。

**Andrej Karpathy：** 我有点怀疑，我得看一看。例如，工业革命前的一些记录不是很好。我有点怀疑它，但我没有强烈的意见。你是说那是一个非常神奇的单一事件。

**Andrej Karpathy：** 你是说也许会有另一个事件就像那样，非常神奇。它会打破范式，等等。我实际上不认为……

**采访者：** 工业革命的关键在于它并不神奇。如果你只是放大看，你在 1770 年或 1870 年看到的并不是有什么关键发明。

**采访者：** 但与此同时，你确实将经济推向了一个进展快得多的机制，指数增长了 10 倍。我对 AI 也有类似的预期，并不是说会有一个单一时刻我们做出了关键发明。而是一个正在被解锁的积压（overhang）。就像也许有一个新能源。

**采访者：** 有某种解锁——在这种情况下，某种认知能力——还有积压的认知工作要做。

**Andrej Karpathy：** 没错。

**采访者：** 你预计当这项新技术跨过门槛时，那个积压会被填补。也许思考它的一种方式是，纵观历史，很多增长来自于人们想出点子，然后人们在外面做事来执行这些点子并制造有价值的产出。

**采访者：** 在这段时间的大部分时间里，人口一直在爆炸。这一直在推动增长。在过去的 50 年里，人们争论增长停滞了。前沿国家的人口也停滞了。我认为我们回到了人口指数增长导致产出超指数增长的情况。

## 智能的进化起源

**Andrej Karpathy：** 真的很难说。我理解那个观点。我直觉上感觉不到那个观点。

**采访者：** 你向我推荐了尼克·莱恩（Nick Lane）的书。在此基础上，我也发现它超级有趣，我采访了他。我有关于思考智能和进化历史的一些问题。

**采访者：** 既然你在过去 20 年做 AI 研究，你也许对什么是智能、开发它需要什么有了更切实的认识。你会因此对进化只是自发地偶然发现了它感到更多还是更少的惊讶？

**Andrej Karpathy：** 我喜欢尼克·莱恩的书。我在来这里的路上还在听他的播客。关于智能及其进化，它是非常非常近期的。我很惊讶它进化出来了。

**Andrej Karpathy：** 思考外面所有的世界真是太迷人了。假设有一千个像地球一样的行星，它们看起来是什么样的。我想尼克·莱恩在这里谈论了一些最早期的部分。他预计大多数行星上会有非常相似的生命形式，粗略地说，也就是细菌之类的东西。那里有一些断裂点。

**Andrej Karpathy：** 直觉上我觉得智能的进化应该是一个相当罕见的事件。也许你应该基于某样东西存在了多久。如果细菌存在了 20 亿年而什么都没发生，那么变成真核生物可能相当困难，因为细菌在地球进化或历史中出现得相当早。

**Andrej Karpathy：** 我们有动物多久了？也许几亿年，到处跑、爬行等等的多细胞动物。那大概是地球寿命的 10%。也许在那个时间尺度上并不太棘手。

**Andrej Karpathy：** 但对我来说，直觉上，它的发展仍然令人惊讶。我也许会预期只有很多做动物类事情的动物类生命形式。你能得到某种创造文化和知识并积累它的东西，这让我很惊讶。

**采访者：** 有几个有趣的后续问题。如果你相信萨顿（Sutton）的观点，即智能的关键是动物智能……他说的那句话是“如果你到了松鼠，你就完成了通往 AGI 的大部分路程。”

**采访者：** 我们在 6 亿年前寒武纪大爆发之后立刻就有了松鼠智能。似乎引发那次事件的是 6 亿年前的氧化事件。但立刻，制造松鼠智能的智能算法就在那里了。

**采访者：** 这暗示动物智能就是那样的。一旦你在环境中有氧气，你有真核生物，你就可以得到算法。也许进化这么快就偶然发现它是一个意外，但我不知道这是否表明最终它会相当简单。

**Andrej Karpathy：** 这很难说。你可以根据某样东西存在了多久或者感觉某样东西被瓶颈限制了多久来判断。尼克·莱恩非常擅长描述细菌和古菌中这种非常明显的瓶颈。

**Andrej Karpathy：** 20 亿年来，什么都没发生。生物化学极其多样化，然而没有什么生长成为动物。20 亿年。我不知道我们是否在动物和智能方面确切地看到了那种等价物，就你的观点而言。我们也可以看看我们认为某些智能单独涌现了多少次。

**Andrej Karpathy：** 那是一个非常值得调查的事情。关于那个的一个想法。有人科动物智能，然后有鸟类智能。乌鸦等极其聪明，但它们的大脑部分相当不同，我们没有那么多共同点。这是智能可能涌现了几次的一个轻微迹象。

**Andrej Karpathy：** 在那种情况下，你会预期它更频繁。

**采访者：** 一位前嘉宾 Gwern 和 Carl Shulman，他们对此提出了一个非常有趣的观点。他们的观点是，人类和灵长类动物拥有的可扩展算法，在鸟类中也出现了，也许其他时候也出现过。

**采访者：** 但人类找到了一个进化利基，这个利基奖励智能的边际增加，并且拥有一个可扩展的大脑算法，可以实现这些智能的增加。

**采访者：** 例如，如果鸟有一个更大的大脑，它就会从空中掉下来。就其大脑大小而言，它非常聪明，但它不在一个奖励大脑变大的利基中。这也许类似于一些非常聪明的……

**Andrej Karpathy：** 像海豚？

**采访者：** 没错，人类，我们有双手奖励学习如何使用工具。我们可以外化消化，给大脑更多能量，这启动了飞轮。

**Andrej Karpathy：** 还有可以利用的东西。我猜如果我是海豚会更难。你怎么有火？在水里，你可以做的事情的宇宙可能比你在陆地上可以做的要低，仅仅是在化学上。

**Andrej Karpathy：** 我确实同意关于这些利基和什么被激励的观点。我仍然觉得这很神奇。我会预期事情会卡在拥有更大肌肉的动物身上。

**Andrej Karpathy：** 经历智能是一个真正令人着迷的突破点。Gwern 的说法是，之所以这么难，是因为它在一种情况之间是一条非常紧密的线：有些东西非常重要以至于不值得将完全正确的电路直接蒸馏回你的 DNA 中，与它不够重要根本不需要学习之间。

**采访者：** 它必须是某种激励在一生中构建算法来学习的东西。

## AI 文化与自我对弈

**Andrej Karpathy：** 你必须激励某种适应性。你想要不可预测的环境，这样进化就不能把你的算法烘焙进你的权重里。

**Andrej Karpathy：** 很多动物在这个意义上是预先烘焙好的。人类必须在出生后的测试时间弄清楚。你想要这些变化非常迅速的环境，在那里你无法预见什么会有效。你创造智能以便在测试时间弄清楚。

**采访者：** Quintin Pope 有一篇有趣的博文，他说他不预期急剧起飞的原因是人类经历了急剧起飞，60,000 年前我们似乎拥有了今天的认知架构。10,000 年前，农业革命，现代性。

**采访者：** 在那 50,000 年里发生了什么？你必须构建这个文化脚手架，在那里你可以跨代积累知识。这是一种在我们进行 AI 训练的方式中免费存在的能力。

**采访者：** 在许多情况下，它们确实是被蒸馏的。如果你重新训练一个模型，它们可以在彼此身上训练，它们可以在相同的预训练语料库上训练，它们实际上不必从头开始。

**采访者：** 某种意义上说，人类花了很长时间才让这个文化循环运转起来，但在我们进行 LLM 训练的方式中，它只是免费的。

**Andrej Karpathy：** 是也不是。因为 LLM 并没有真正拥有文化的等价物。也许我们给了它们太多，激励它们不创造它或类似的东西。但文化的创造、书面记录以及彼此之间传递笔记，我认为目前 LLM 没有与之相当的东西。LLM 目前并没有真正的文化，我会说这是障碍之一。

**采访者：** 你能给我一些 LLM 文化可能是什么样子的感觉吗？

**Andrej Karpathy：** 在最简单的情况下，它会是一个巨大的草稿纸，LLM 可以编辑，当它在阅读东西或在帮助工作时，它在为自己编辑草稿纸。

**Andrej Karpathy：** 为什么一个 LLM 不能为其他 LLM 写一本书？那会很酷。为什么其他 LLM 不能读这个 LLM 的书并受到启发或震惊或类似的东西？没有任何与之相当的东西。

**采访者：** 有趣。你预期那种事情什么时候开始发生？还有多智能体系统和一种独立的 AI 文明和文化？

**Andrej Karpathy：** 在多智能体领域有两个强大的想法，都没有真正被认领或什么的。第一个我会说是文化和 LLM 拥有为自己目的而增长的知识库。

**Andrej Karpathy：** 第二个看起来更像自我对弈（self-play）这个强大的想法。在我看来这极其强大。进化有很多竞争驱动智能和进化。在 AlphaGo 中，更算法化地说，AlphaGo 与自己对弈，这就是它如何学会在围棋上变得非常厉害的。

**Andrej Karpathy：** 没有自我对弈 LLM 的等价物，但我会预期那也会存在。还没有人做过。为什么一个 LLM 不能例如创造一堆另一个 LLM 正在学习解决的问题？然后 LLM 总是试图提供越来越难的问题，诸如此类。有很多组织它的方式。

**Andrej Karpathy：** 这是一个研究领域，但我还没有看到任何令人信服地声称这两种多智能体改进的东西。我们主要处于单个个体智能体的领域，但这将会改变。在文化领域也是，我也会把组织归入其中。我们也没有令人信服地看到类似那样的东西。这就是为什么我们还很早。

**采访者：** 你能找出阻止 LLM 之间这种协作的关键瓶颈吗？

**Andrej Karpathy：** 也许我会这样说，其中一些类比有效，它们本不应该有效，但不知何故，非常显著地，它们有效。很多较小的模型，或者较笨的模型，非常像幼儿园学生，或者是小学生或高中生。

## 自动驾驶的教训：演示与产品的差距

**Andrej Karpathy：** 不知何故，我们仍然没有毕业到足以让这些东西接管的地步。我的 Claude Code 或 Codex，它们仍然感觉像这个小学年级的学生。我知道它们可以通过博士测验，但它们在认知上仍然感觉像幼儿园或小学生。

**Andrej Karpathy：** 我不认为它们能创造文化，因为它们还是孩子。它们是学者症候群的孩子。它们对所有这些东西有完美的记忆。它们可以令人信服地创造各种看起来非常好的“泔水”。但我仍然认为它们并不真的知道自己在做什么，它们并不真的拥有跨越所有这些我们仍然必须收集的小复选框的认知。

**采访者：** 你谈到过你在 2017 年到 2022 年在特斯拉领导自动驾驶。你亲眼目睹了从很酷的演示到现在的成千上万辆车在外面实际自主驾驶的进展。为什么那花了十年？那段时间发生了什么？

**Andrej Karpathy：** 我几乎会立即反驳的一点是，这甚至还没接近完成，在很多方面我稍后会讲到。

**Andrej Karpathy：** 自动驾驶非常有趣，因为它绝对是我获得很多直觉的地方，因为我在上面花了五年。它有这整个历史，自动驾驶的第一个演示一直追溯到 1980 年代。你可以看到 CMU 在 1986 年的一个演示。

**Andrej Karpathy：** 有一辆卡车在路上自己驾驶。快进。当我加入特斯拉时，我有一个非常早期的 Waymo 演示。它基本上在 2014 年或那左右给了我一个完美的驾驶，所以十年前就是一个完美的 Waymo 驾驶。

**Andrej Karpathy：** 它带我们在帕洛阿尔托等地转了一圈，因为我有一个朋友在那里工作。我认为它非常接近了，然后它仍然花了很长时间。

**Andrej Karpathy：** 对于某些类型的任务和工作等等，有一个非常大的演示到产品的差距，演示非常容易，但产品非常难。特别是在像自动驾驶这样的情况下，失败的成本太高了。

**Andrej Karpathy：** 许多行业、任务和工作也许没有那个属性，但是当你确实有那个属性时，那肯定会增加时间线。例如，在软件工程中，我确实认为那个属性是存在的。对于很多氛围编程，它不存在。

**Andrej Karpathy：** 但如果你在写实际的生产级代码，那个属性应该存在，因为任何形式的错误都会导致安全漏洞或类似的东西。数百万和数亿人的个人社会安全号码被泄露或类似的东西。

**Andrej Karpathy：** 所以在软件方面，人们应该小心，有点像在自动驾驶中。在自动驾驶中，如果事情出错了，你可能会受伤。有更糟糕的结果。但在软件中，事情可能有多糟糕几乎是无界的。我确实认为它们共享那个属性。

**Andrej Karpathy：** 耗费长时间的原因以及思考它的方式是，这是一场关于“9”的行军（march of nines，指可靠性从90%到99%再到99.9%等）。每一个“9”都是恒定数量的工作。每一个“9”都是相同数量的工作。当你得到一个演示，有些东西 90% 的时间有效，那只是第一个“9”。

**Andrej Karpathy：** 然后你需要第二个“9”，第三个“9”，第四个“9”，第五个“9”。当我在特斯拉的大约五年里，我们经历了大概三个“9”或两个“9”。我不知道是多少，但是多次迭代的“9”。还有更多的“9”要走。这就是为什么这些事情花这么长时间。

**Andrej Karpathy：** 这对我来说绝对是形成性的，看到一些只是演示的东西。我对演示非常不感冒。每当我看到任何东西的演示，我都非常不感冒。如果是有人为了展示而炮制的演示，那就更糟了。

**Andrej Karpathy：** 如果你可以与它互动，那就好一点。但即便如此，你还没完成。你需要实际的产品。当它接触到现实以及所有这些需要修补的不同行为口袋时，它将面临所有这些挑战。我们将看到所有这些东西上演。这是一场关于“9”的行军。每个“9”都是恒定的。演示是令人鼓舞的。这仍然是大量的工作要做。这是一个关键的安全领域，除非你在做氛围编程，那是很好很有趣等等。这就是为什么这从那个角度也强化了我的时间线。

**采访者：** 听到你说你需要从软件中获得的安全保证与自动驾驶并无不同，这非常有趣。人们经常会说自动驾驶之所以花这么长时间是因为失败的成本太高了。

**采访者：** 人类平均每 400,000 英里或每七年犯一次错。如果你必须发布一个至少七年不能犯错的编码智能体，部署起来会难得多。但你的观点是，如果你每七年犯一个灾难性的编码错误，比如破坏了一些重要系统……

**Andrej Karpathy：** 很容易做到。事实上，就挂钟时间而言，它会远少于七年，因为你一直在像那样输出代码。就 token 而言，它会是七年。但就挂钟时间而言……在某些方面，这是一个更难的问题。自动驾驶只是人们做的成千上万件事中的一件。我想这几乎就像一个单一的垂直领域。而当我们谈论通用软件工程时，它甚至更多……有更多的表面积。

**采访者：** 人们对这个类比还有另一个反对意见，那就是对于自动驾驶，花费大部分时间的是解决拥有基本感知的问题，这种感知是稳健的，构建表示，并拥有一个具有一些常识的模型，以便当它看到稍微超出分布的东西时可以泛化。

**采访者：** 如果有人在路上这样挥手，你不需要为此训练。这东西会对如何应对那样的事情有一些理解。这些是我们今天通过 LLM 或 VLM 免费获得的东西，所以我们不必解决这些非常基本的表示问题。

**采访者：** 所以现在跨不同领域部署 AI 会有点像用当前模型将自动驾驶汽车部署到不同的城市，这很难，但不像是一个长达 10 年的任务。

**Andrej Karpathy：** 我不是 100% 确定我是否完全同意这一点。我不知道我们免费获得了多少。在理解我们正在获得什么方面仍然有很多差距。我们肯定在一个单一实体中获得了更可泛化的智能，而自动驾驶是一个非常特殊的任务，需要……

**Andrej Karpathy：** 在某种意义上，构建一个特殊用途的任务也许在某种意义上更难，因为它不是从你正在大规模做的更通用的事情中掉出来的，如果这说得通的话。

**Andrej Karpathy：** 但这个类比仍然没有完全产生共鸣，因为 LLM 仍然相当容易犯错，它们有很多差距仍然需要填补。我不认为我们在某种意义上完全开箱即用地获得了神奇的泛化。

**Andrej Karpathy：** 我想回到自动驾驶汽车仍然远未完成的另一方面。部署相当少。即使是 Waymo 等也只有很少的车。

**Andrej Karpathy：** 他们这样做粗略地说是因为它们不经济。他们构建了一些生活在未来的东西。他们不得不把未来拉回来，但他们不得不让它变得不经济。有所有这些成本，不仅是那些汽车及其运营和维护的边际成本，还有整个事情的资本支出（capex）。

**Andrej Karpathy：** 让它变得经济对他们来说仍然是一个艰难的过程。而且，当你看着这些车，没有人驾驶，我实际上认为这有点欺骗性，因为有非常精细的远程操作中心，人们在与这些车保持某种循环。我没有它的全部范围，但有比你预期的更多的人在循环中。

**Andrej Karpathy：** 有人在外面的某个地方从天空传送进来。我不知道他们是否完全在驾驶循环中。有些时候他们是，但他们肯定参与其中，而且有人。在某种意义上，我们实际上并没有移除人，我们把他们移到了你看不到的地方。

**Andrej Karpathy：** 正如你提到的，我仍然认为从环境到环境会有一些工作。要让自动驾驶成为现实仍然有挑战。但我确实同意它肯定跨越了一个门槛，感觉它是真实的，除非它是真正的远程操作。例如，Waymo 不能去城市的所有不同部分。我的怀疑是那是你得不到好信号的城市部分。

**Andrej Karpathy：** 总之，我对技术栈一无所知。我只是在瞎编。你在特斯拉领导了五年的自动驾驶。抱歉，我对 Waymo 的具体细节一无所知。顺便说一句，我爱 Waymo，我一直坐它。

**Andrej Karpathy：** 我只是认为人们有时对一些进展有点太天真了，仍然有大量的工作要做。在我看来，特斯拉采取了一种更可扩展的方法，团队做得非常好。我对这事情会如何发展有点像是在公开预测。

**Andrej Karpathy：** Waymo 有一个早期的开始，因为你可以打包很多传感器。但我确实认为特斯拉正在采取更可扩展的策略，它看起来会更像那样。所以这仍然必须上演并且还没有。但我不想把自动驾驶谈论成花了十年的事情，因为它还没花完，如果这说得通的话。

**Andrej Karpathy：** 因为第一，开始是在 1980 年而不是 10 年前，第二，结束还没有到。结束还没有到，因为当我们谈论自动驾驶时，通常在我脑海里是大规模的自动驾驶。人们不需要考驾照，等等。

**采访者：** 我很好奇这两个类比可能不同的其他两种方式。我之所以特别好奇这一点，是因为 AI 部署的速度有多快，它在早期有多大价值，这可能是目前世界上最重要的问题。如果你试图模拟 2030 年的样子，这是你应该有一些理解的问题。

**采访者：** 你可能会想到的另一件事是，第一，你有这个自动驾驶的延迟要求。我不知道实际模型是什么，但我假设它是像几千万参数或什么的，这对于使用 LLM 的知识工作来说不是必要的约束。

**采访者：** 也许对于计算机使用之类的可能是。但另一个大的，也许更重要的是，在这个资本支出问题上。是的，提供模型的额外副本有额外的成本，但会话的运营支出（opex）相当低，你可以将 AI 的成本分摊到训练运行本身，这取决于推理扩展如何进行等等。

**采访者：** 但这肯定不像为了服务另一个模型实例而建造一辆全新的汽车那么多。所以更广泛部署的经济性要有利得多。

**Andrej Karpathy：** 我认为是那样的。如果你坚持在比特（bits）的领域，比特比任何接触物理世界的东西都要容易一百万倍。我绝对承认这一点。比特是完全可变的，可以以非常快的速度任意重新洗牌。

**Andrej Karpathy：** 你也会预期在这个行业等等有更快的适应。第一个是什么？

**采访者：** 延迟要求及其对模型大小的影响？

**Andrej Karpathy：** 我认为那大致是正确的。我也认为如果我们谈论大规模的知识工作，实际上会有一些延迟要求，因为我们将不得不创造大量的计算并为此服务。

**Andrej Karpathy：** 我非常简短地想谈的最后一个方面是所有其余的部分。社会怎么看它？法律后果是什么？它在法律上如何运作？它在保险方面如何运作？这些层和方面是什么？

## 算力建设与泡沫担忧

**Andrej Karpathy：** 人们在 Waymo 上放一个锥桶的等价物是什么？将会有所有这些的等价物。所以我感觉自动驾驶是一个很好的类比，你可以从中借用东西。

**Andrej Karpathy：** 车里的锥桶的等价物是什么？隐藏起来的远程操作工人的等价物是什么，以及所有这些方面。

**采访者：** 你对这意味着什么有任何看法吗，关于当前的 AI 建设，这将在一年或两年内使世界上可用的计算量增加 10 倍，也许到本十年末增加 100 倍以上。如果 AI 的使用率低于一些人天真的预测，这是否意味着我们过度建设了计算，还是那是一个单独的问题？

**Andrej Karpathy：** 有点像铁路发生的事情。

**采访者：** 比如什么，抱歉？是铁路还是？

**Andrej Karpathy：** 是的，它是。是的。有历史先例。或者是电信行业？预先铺设了十年后才到来的互联网，并在 90 年代末在电信行业制造了整个泡沫。

**Andrej Karpathy：** 我明白我在这里听起来非常悲观。我实际上是乐观的。我认为这会奏效。我认为这是可处理的。我听起来悲观只是因为当我去我的推特时间线时，我看到所有这些对我来说毫无意义的东西。

**Andrej Karpathy：** 那之所以存在有很多原因。很多老实说只是为了融资。这只是激励结构。很多可能是融资。很多只是注意力，把互联网上的注意力转化为金钱，诸如此类。

**Andrej Karpathy：** 有很多那样的事情发生，我只是对此做出反应。但我总体上仍然非常看好技术。我们将解决所有这些问题。有了快速的进展。我不知道是否存在过度建设。我认为我们将能够吞噬据我了解正在建设的东西。

**Andrej Karpathy：** 例如，Claude Code 或 OpenAI Codex 等等一年前甚至不存在。是这样吗？这是一项不存在的神奇技术。将会有巨大的需求，正如我们在 ChatGPT 中已经看到的需求等等。所以我不知道是否存在过度建设。

## Eureka 项目：AI 时代的教育愿景

**Andrej Karpathy：** 我只是对人们继续错误地说的非常快的时间线做出反应。在我从事 AI 的 15 年里，我听过很多很多次，非常有声望的人一直把这个搞错。我希望这能得到适当的校准，其中一些问题也有地缘政治影响等等。

**Andrej Karpathy：** 我不想让人们在那个领域犯错。我确实希望我们立足于技术是什么和不是什么的现实中。

**采访者：** 让我们谈谈教育和 Eureka。你可以做的一件事是创办另一个 AI 实验室，然后试图解决那些问题。我很好奇你现在在忙什么，为什么不是 AI 研究本身？

**Andrej Karpathy：** 我想我会这样说，我对 AI 实验室正在做的事情感到某种程度的决定论。我觉得我可以在那里帮忙，但我不知道我是否会独特地改进它。

**Andrej Karpathy：** 我个人最大的恐惧是很多这种事情发生在人类的一边，而人类因此被剥夺了权力。我不只是关心我们将要建造的所有戴森球以及 AI 将以完全自主的方式建造的所有东西，我关心人类会发生什么。

**Andrej Karpathy：** 我希望人类在未来过得好。我觉得这是我可以比前沿实验室的渐进式改进更独特地增加价值的地方。我最害怕的是像《机器人总动员》（WALL-E）或《蠢蛋进化论》（Idiocracy）或类似电影中描述的事情，人类处于这些东西的边缘。

**Andrej Karpathy：** 我希望人类在这个未来变得更好，好得多。对我来说，这是通过教育可以实现的。

**采访者：** 那么你在那里做什么？

**Andrej Karpathy：** 我能描述它的最简单方式是我们正试图建立星际舰队学院（Starfleet Academy）。我不知道你是否看过《星际迷航》。

**采访者：** 我没看过。

**Andrej Karpathy：** 星际舰队学院是这所用于前沿技术、建造宇宙飞船以及让学员毕业成为这些宇宙飞船的飞行员等的精英机构。所以我只是想象一所用于技术知识的精英机构，一种非常与时俱进的学校和一流的机构。

**采访者：** 我有一类问题想问你，是关于解释如何教好技术或科学内容，因为你是这方面的世界大师之一。我很好奇你如何为你已经在 YouTube 上发布的内容思考这个问题，而且，如果它有任何不同的话，你如何为 Eureka 思考这个问题。

**Andrej Karpathy：** 关于 Eureka，对我来说关于教育的一件非常迷人的事情是，我确实认为随着 AI 的加入，教育将发生相当根本的变化。它必须在某种程度上被重新连线和改变。我仍然认为我们还很早。

**Andrej Karpathy：** 会有很多人尝试做显而易见的事情。拥有一个 LLM 并问它问题。做所有你现在会通过提示做的基本事情。这很有帮助，但对我来说仍然感觉有点像“泔水”。我想正确地做这件事，我认为能力还没有达到我想要的水平。

**Andrej Karpathy：** 我想要的是一个真正的导师体验。我脑海中一个突出的例子是我最近在学韩语，也就是语言学习。我经历了一个阶段，我在互联网上自学韩语。

**Andrej Karpathy：** 我经历了一个阶段，我在韩国参加了一个小班，和一堆其他人一起上韩语课，这真的很有趣。我们有一个老师和大约 10 个人在学韩语。然后我转到了一对一的导师。

**Andrej Karpathy：** 我想让我着迷的是，我认为我有一个非常好的导师，但仅仅是思考这个导师为我做了什么，那种体验有多不可思议，以及我想最终构建的东西门槛有多高。从非常短的对话中，她立刻明白了我作为一个学生处于什么位置，我知道什么和不知道什么。

**Andrej Karpathy：** 她能够探测确切类型的问题或事情来理解我的世界模型。现在没有 LLM 会为你 100% 做到这一点，甚至都不接近。但如果导师好的话，他们会这样做。一旦她理解了，她真的为我提供了我在当前能力范围内所需要的所有东西。

**Andrej Karpathy：** 我需要总是受到适当的挑战。我不能面对太难或太琐碎的东西，而导师真的擅长为你提供恰到好处的东西。我觉得我是学习的唯一约束。我总是被给予完美的信息。我是唯一的障碍。

**Andrej Karpathy：** 感觉很好，因为我是唯一的障碍。并不是我找不到知识或者它没有被正确解释等等。只是我的记忆能力等等。这就是我想要给人们的。你怎么自动化那个？

**Andrej Karpathy：** 非常好的问题。以目前的能力，你做不到。这就是为什么我认为实际上现在不是构建这种 AI 导师的正确时机。我仍然认为它是一个有用的产品，很多人会构建它，但门槛太高了，能力还不到位。

**Andrej Karpathy：** 即使在今天，我会说 ChatGPT 是一个极其有价值的教育产品。但对我来说，看到门槛有多高真是太迷人了。当我和她在一起时，我几乎觉得没办法构建这个。

**采访者：** 但你正在构建它，对吧？

**Andrej Karpathy：** 任何有过真正好导师的人都会想，“你怎么构建这个？”我在等待那种能力。我做了一些计算机视觉的 AI 咨询。很多时候，我带给公司的价值是告诉他们不要使用 AI。

**Andrej Karpathy：** 我是 AI 专家，他们描述了问题，我说，“不要用 AI。”这是我的增值。我觉得现在在教育领域也是一样，我觉得对于我心中的想法，时机还没到，但时机终会到来。

**Andrej Karpathy：** 目前，我正在构建一些看起来也许更传统的东西，有一个物理和数字组件等等。但很明显这在未来应该是什么样子的。

**采访者：** 如果你愿意说的话，你希望今年或明年发布的东西是什么？

**Andrej Karpathy：** 我正在构建第一门课程。我想有一门非常非常好的课程，你去学习 AI 的显而易见的最先进的目的地。那只是我熟悉的领域，所以这是一个非常好的第一个产品，可以做得非常好。这就是我正在构建的。Nanochat，你简要提到的，是 LLM101N 的一个顶点项目，这是我正在构建的一门课。

**Andrej Karpathy：** 那是其中非常大的一块。但现在我必须构建所有的中间环节，然后我必须雇用一个小型的助教团队等等，构建整个课程。

**Andrej Karpathy：** 我想说的另一件事是，很多时候，当人们思考教育时，他们更多地思考我会说是传播知识的较软组件。我心中有一些非常硬核和技术性的东西。

**Andrej Karpathy：** 在我看来，教育是构建通往知识的坡道这一非常困难的技术过程。在我看来，nanochat 是通往知识的坡道，因为它非常简单。它是超级简化的全栈东西。如果你把这个工件给某人，他们浏览它，他们就在学习大量的东西。

**Andrej Karpathy：** 它给了你很多我所谓的每秒尤里卡（eurekas per second），也就是每秒的理解。那就是我想要的，大量的每秒尤里卡。所以对我来说，这是一个技术问题，即我们如何构建这些通往知识的坡道。所以我几乎认为 Eureka 也许与一些前沿实验室或那里正在进行的一些工作没有太大不同。

**Andrej Karpathy：** 我想弄清楚如何非常高效地构建这些坡道，以便人们永远不会被卡住，一切总是不会太难也不会太琐碎，你有恰到好处的材料来进步。

**采访者：** 你在短期内想象的是，与其让导师能够探测你的理解，如果你有足够的自我意识能够探测自己，你就永远不会被卡住。你可以在与助教交谈或与 LLM 交谈和查看参考实现之间找到正确的答案。

**采访者：** 听起来自动化或 AI 不是一个重要的部分。到目前为止，这里的大阿尔法是你将解释 AI 的能力编纂进课程源材料中。

**Andrej Karpathy：** 那从根本上就是课程。你必须始终根据行业中存在的能力校准自己。很多人会追求仅仅问 ChatGPT 等等。

**Andrej Karpathy：** 但我认为现在，例如，如果你去 ChatGPT 说，教我 AI，没办法。它会给你一些“泔水”。AI 现在永远写不出 nanochat。但 nanochat 是一个非常有用的中间点。

**Andrej Karpathy：** 我正在与 AI 合作创造所有这些材料，所以 AI 从根本上仍然非常有帮助。早些时候，我在斯坦福构建了 CS231n，我认为那是斯坦福的第一门深度学习课程，后来变得非常受欢迎。

**Andrej Karpathy：** 那时构建 231n 和现在构建 LLM101N 的区别是非常明显的。我对现在的 LLM 感到非常有力量，但我非常在循环中。它们帮我构建材料，我快得多。

**Andrej Karpathy：** 它们在做很多无聊的事情，等等。我觉得我开发课程的速度快得多，它是注入了 LLM 的，但还没有达到它可以创造性地创造内容的地步。我还在那里做这件事。棘手之处总是根据现有的东西校准自己。

**采访者：** 当你想象几年后通过 Eureka 可以获得什么时，似乎最大的瓶颈将是在一个又一个领域找到像 Karpathy 这样的人，他们可以将自己的理解转化为这些坡道。

**Andrej Karpathy：** 它会随着时间的推移而改变。现在，这将是雇用教员来帮助与 AI 携手工作，并且可能是一个团队来构建最先进的课程。随着时间的推移，也许一些助教可以变成 AI。

**Andrej Karpathy：** 你只需拿走所有的课程材料，然后我认为当学生有更基本的问题或类似的问题时，你可以为他们提供一个非常好的自动化助教。但我认为你需要教员来负责课程的整体架构并确保它合适。

**Andrej Karpathy：** 所以我看到了这将如何演变的进程。也许在未来的某个时刻，我甚至没有那么有用，AI 做的设计大部分比我能做的要好得多。但我仍然认为这需要一些时间来上演。

**采访者：** 你是在想象拥有其他领域专业知识的人随后贡献课程，还是你觉得考虑到你对你想如何教学的理解，你是设计内容的人这一点对愿景至关重要？

**采访者：** 萨尔·可汗（Sal Khan）正在解说可汗学院的所有视频。你在想象类似的事情吗？

**Andrej Karpathy：** 不，我会雇用教员，因为有些领域我不是专家。这是最终为学生提供最先进体验的唯一途径。我确实预计我会雇用教员，但我可能会在 AI 领域坚持一段时间。

**Andrej Karpathy：** 对于目前的能力，我心中确实有一些比人们可能预期的更传统的东西。当我构建星际舰队学院时，我确实可能想象一个物理机构，也许在那之下一层是一个数字产品，那不是当某人全职亲自进来，我们从头到尾通过材料并确保你理解它时你会得到的最先进体验。

**Andrej Karpathy：** 那是物理产品。数字产品是互联网上的一堆东西，也许还有一些 LLM 助手。它在下一层有点噱头，但至少它可以让 80 亿人访问。

**采访者：** 我认为你基本上是在利用今天可用的工具从第一性原理发明大学，并且只选择那些有动力和兴趣真正接触材料的人。

**Andrej Karpathy：** 将会有很多不仅仅是教育还有再教育。我很想在那里帮忙，因为工作可能会发生很大的变化。例如，今天很多人试图专门在 AI 方面提升技能。我认为在这方面这是一门非常好的课程。

**Andrej Karpathy：** 在动力方面，在 AGI 之前，动力很简单解决，因为人们想赚钱。这就是你今天在这个行业赚钱的方式。AGI 之后可能更有趣，因为如果一切都被自动化了，任何人都没有事做，为什么还有人去上学？

**Andrej Karpathy：** 我经常说 AGI 之前的教育是有用的。AGI 之后的教育是有趣的。以类似的方式，人们今天去健身房。我们不需要他们的体力来操纵重物，因为我们有机器做那个。他们仍然去健身房。

**Andrej Karpathy：** 他们为什么去健身房？因为它很有趣，它很健康，而且当你有六块腹肌时你看起来很性感。对于人类来说，在一个非常深刻的、心理的、进化的意义上，这样做是有吸引力的。

**Andrej Karpathy：** 教育将以同样的方式上演。你会去上学就像你去健身房一样。现在，没有那么多人学习，因为学习很难。你会从材料上反弹。有些人克服了那个障碍，但对大多数人来说，这很难。

**Andrej Karpathy：** 这是一个需要解决的技术问题。这是做我的导师在我学韩语时为我做的事情的技术问题。它是可处理的和可构建的，有人应该构建它。它将使学习任何东西变得微不足道且令人向往，人们会为了乐趣而做，因为它是微不足道的。

**Andrej Karpathy：** 如果我对任何任意知识都有那样的导师，学习任何东西都会变得容易得多，人们会去做。他们会出于与去健身房相同的原因去做。

**采访者：** 这听起来不同于使用……所以在 AGI 之后，你把它作为娱乐或自我完善。但这听起来你也有一种愿景，即这种教育与让人类保持对 AI 的控制有关。这听起来不同。对某些人来说是有趣的，但对另一些人来说是赋权吗？你是怎么想的？

**Andrej Karpathy：** 我确实认为最终这是一场必输的游戏，如果这说得通的话。

**Andrej Karpathy：** 从长远来看是这样。长远来看，这可能比行业中大多数人想的要长，这是一场必输的游戏。我确实认为人们可以走得很远，我们几乎还没触及一个人能走多远的表面。

**Andrej Karpathy：** 那只是因为人们从太容易或太难的材料上反弹。人们将能够走得更远。任何人都会说五种语言，因为为什么不呢？因为它太微不足道了。任何人都会知道本科的所有基本课程，等等。

**采访者：** 现在我理解了这个愿景，这非常有趣。它在健身房文化中有一个完美的类比。我不认为 100 年前有人会肌肉发达。没有人能够自发地推举两片或三片杠铃片或类似的东西。

**采访者：** 现在这很常见，因为这种在健身房系统地训练和举重，或者系统地训练以便能够跑马拉松的想法，这是大多数人类不会自发拥有的能力。你在想象跨许多不同领域的学习也有类似的事情，更加强烈、深入、快速。

**Andrej Karpathy：** 没错。我有点含蓄地押注于人性的某种永恒性。做所有这些事情将是令人向往的，我认为人们会像几千年来那样仰视它。这在历史上有一些证据。

**Andrej Karpathy：** 如果你看，例如，贵族，或者你看古希腊或类似的地方，每当你有一些在某种意义上是后 AGI 的小口袋环境时，人们就会花很多时间以某种方式蓬勃发展，无论是身体上还是认知上。

**Andrej Karpathy：** 我对那个前景感觉良好。如果这是错误的，我错了，我们最终进入了一个《机器人总动员》或《蠢蛋进化论》的未来，那么即使有戴森球我也不在乎。这是一个糟糕的结果。我真的关心人类。每个人都必须在某种意义上变得超人。

**采访者：** 这仍然是一个并不能使我们……这就像文化世界，对吧？你不能从根本上通过你自己的劳动或认知单独改变技术的轨迹或影响决策。

**采访者：** 也许你可以影响决策，因为 AI 正在征求你的批准，但这并不是因为我发明了什么或者我想出了一个新的设计，我就真的在影响未来。

**Andrej Karpathy：** 也许。我认为会有一个过渡期，如果我们理解很多东西，我们将能够处于循环中并推进事情。从长远来看，这可能会消失。

**Andrej Karpathy：** 它甚至可能变成一种运动。现在你有力量举重运动员在这个方向上走极端。认知时代的力量举重是什么？也许是人们真的试图把知道东西变成奥林匹克运动。

**Andrej Karpathy：** 如果你有一个完美的 AI 导师，也许你可以走得非常远。我觉得今天的天才们几乎还没触及人类思维能做什么的表面，我认为。

**采访者：** 我喜欢这个愿景。我也觉得你最有产品市场契合度的人是我，因为我的工作涉及每周必须学习不同的科目，我很兴奋。

**Andrej Karpathy：** 我也是类似的，就此而言。很多人，例如，讨厌学校并想离开它。我真的很喜欢学校。我喜欢学习东西，等等。我想留在学校。

**Andrej Karpathy：** 我一直待到博士，然后他们不让我待更久了，所以我去了工业界。粗略地说，我喜欢学习，即使是为了学习而学习，但我也喜欢学习，因为这是一种赋权形式，是有用和富有成效的。

**采访者：** 你还提出了一个微妙的观点，我想把它说清楚。这就目前的在线课程而言，为什么它们还没有使我们能够让每个人都知道一切？它们只是太依赖动力了，因为没有明显的入口，而且很容易被卡住。

**采访者：** 如果你有这个东西代替——就像一个真正好的人类导师——从动力的角度来看，这将是一个巨大的解锁。

**Andrej Karpathy：** 我这么认为。从材料上反弹感觉很糟糕。感觉很糟糕。你在某件事上投入了大量时间却没有结果，或者因为你得到的东西太容易或太难而完全无聊，你会得到负面奖励。当你做对了，学习感觉很好。这是一个需要解决的技术问题。

**Andrej Karpathy：** 有一段时间，这将是 AI 加人类的合作，在某个时候，也许只是 AI。

## 物理直觉与教学艺术

**采访者：** 我能问一些关于教好的问题吗？如果你必须给另一个你感兴趣领域的教育者建议，让他们制作你做过的那种 YouTube 教程。也许谈论那些你不能通过让人编写代码或其他东西来测试某人技术理解的领域会特别有趣。你会给他们什么建议？

**Andrej Karpathy：** 这是一个相当广泛的话题。我可能有 10-20 个我在半意识状态下做的技巧和窍门。但这很多来自我的物理背景。我真的非常喜欢我的物理背景。

**Andrej Karpathy：** 我有一整套关于每个人都应该在早期学校教育中学习物理的咆哮，因为早期学校教育不是为了以后的工业任务积累知识或记忆。它是关于启动大脑。

**Andrej Karpathy：** 物理独特地最好地启动大脑，因为他们在物理期间让你在大脑中做的一些事情在以后极其有价值。构建模型和抽象的想法，以及理解有一个一阶近似描述了大部分系统，但还有二阶、三阶、四阶项可能存在也可能不存在。

**Andrej Karpathy：** 你正在观察一个非常嘈杂的系统，但有这些你可以抽象出来的基频的想法。当一个物理学家走进教室说，“假设有一头球形奶牛，”每个人都嘲笑那个，但这太棒了。

**Andrej Karpathy：** 这是非常可泛化的辉煌思维，因为奶牛在很多方面可以近似为一个球体。有一本非常好的书，例如，《规模》（Scale）。它来自一位谈论生物学的物理学家。

**Andrej Karpathy：** 也许这也是一本我会推荐阅读的书。你可以得到很多非常有趣的近似值，并绘制动物的比例定律。你可以看它们的心跳之类的，它们与动物的大小等一致。你可以把动物说成是一个体积。

**Andrej Karpathy：** 你可以谈论它的散热，因为你的散热随着表面积增长，也就是平方增长。但你的热量产生或生成是随着立方增长的。所以我只是觉得物理学家拥有所有正确的认知工具来处理世界上的问题解决。

**Andrej Karpathy：** 所以因为那种训练，我总是试图找到一切的一阶项或二阶项。当我观察一个系统或事物时，我脑海中有一团想法或知识的乱麻。我试图找到，重要的是什么？一阶组件是什么？

**Andrej Karpathy：** 我怎么能简化它？我怎么能有一个最简单的东西来展示那个东西，展示它的行动，然后我可以加上其他项？也许我的一个代码库中的例子我认为很好地说明了这一点，叫 micrograd。我不知道你是否熟悉这个。

**Andrej Karpathy：** 所以 micrograd 是 100 行代码，展示了反向传播。你可以用简单的操作如加和乘等创建神经网络。神经网络的乐高积木。你建立一个计算图，你做一个前向传播和一个后向传播来获得梯度。现在，这是所有神经网络学习的核心。

**Andrej Karpathy：** 所以 micrograd 是 100 行相当可解释的 Python 代码，它可以对任意神经网络进行前向和后向传播，但不高效。

**Andrej Karpathy：** 所以 micrograd，这 100 行 Python，是你理解神经网络如何训练所需的一切。其他一切都只是效率。其他一切都是效率。为了获得效率有大量的工作。你需要你的张量，你布局它们，你跨步它们，你确保你的内核正确编排内存移动，等等。

**Andrej Karpathy：** 粗略地说，这都只是效率。但神经网络训练的核心智力部分是 micrograd。它是 100 行。你可以很容易地理解它。它是链式法则的递归应用来推导梯度，这允许你优化任何任意可微函数。

**Andrej Karpathy：** 所以我喜欢找到这些小阶项，把它们放在盘子里并发现它们。我觉得教育是智力上最有趣的事情，因为你有一团理解的乱麻，你试图以一种创造坡道的方式将其布局，其中每件事只取决于它之前的事情。

**Andrej Karpathy：** 我发现这种知识的解开作为一项认知任务在智力上太有趣了。我个人喜欢做这件事，但我只是对试图以某种方式布局事物着迷。也许那对我有帮助。这也让学习体验更有动力。

**采访者：** 你关于 transformer 的教程从二元组（bigrams）开始，字面上是一个从“这是现在的词，或者是前面的词，这是下一个词”的查找表。它字面上只是一个查找表。

**Andrej Karpathy：** 这就是它的本质，是的。

**采访者：** 这是一个如此聪明的方法，从查找表开始，然后到 transformer。每一块都有动机。你为什么要加那个？你为什么要加下一个东西？

**采访者：** 你可以记住注意力公式，但理解每一个部分为什么相关，它解决了什么问题。你在提出解决方案之前先提出了痛点，这多聪明啊？

**Andrej Karpathy：** 你想带学生经历那个进程。有很多其他的小事情让它变得美好、引人入胜和有趣。总是提示学生。

**Andrej Karpathy：** 有很多像那样的小事情很重要，很多好的教育者会这样做。你会怎么解决这个问题？我不会在你猜之前给出解决方案。那是浪费。在你尝试自己想出来之前向你展示解决方案，这有点……我不想骂人，但这有点混蛋。

**Andrej Karpathy：** 因为如果你尝试自己想出来，你会更好地理解行动空间是什么，目标是什么，然后为什么只有这个行动能实现那个目标。你有机会自己尝试，当我给你解决方案时你会感激。它最大化了每个新添加事实的知识量。

**采访者：** 你认为为什么默认情况下，那些在其领域真正的专家往往不擅长向正在起步的人解释它？

**Andrej Karpathy：** 这是知识和专业的诅咒。这是一个真实现象，尽管我尽量避免，但我自己也受其苦。但你认为某些事情理所当然，你无法设身处地地为那些刚起步的新人着想。

**Andrej Karpathy：** 这是普遍存在的，也发生在我身上。有一件事极其有帮助。举个例子，最近有人试图给我看一篇生物学论文，我立刻有了很多糟糕的问题。我做的是我用 ChatGPT 问了这些问题，把论文放在上下文窗口里。

**Andrej Karpathy：** 它解决了一些简单的事情。然后我把这个线程分享给了写那篇论文或从事那项工作的人。我觉得如果他们能看到我有的愚蠢问题，可能会帮助他们在未来更好地解释。

**Andrej Karpathy：** 对于我的材料，如果人们分享他们与 ChatGPT 关于我所创造的东西的愚蠢对话，我会很高兴，因为这真的有助于我再次设身处地为刚起步的人着想。另一个技巧效果惊人地好。

**Andrej Karpathy：** 如果有人写了一篇论文或博客文章或公告，在 100% 的情况下，仅仅是叙述或转录他们如何在午餐时向你解释它，不仅更容易理解，而且实际上也更准确和科学。

**采访者：** 从某种意义上说，人们有一种偏见，即以最抽象、充满行话的方式解释事情，并在解释中心思想之前清嗓子写四段话。但是与一个人一对一交流会迫使你只说那件事。

**Andrej Karpathy：** 只说那件事。我看到了那条推文，我觉得真的很好。我和一堆人分享了它。我注意到这个很多很多次。

**Andrej Karpathy：** 最突出的例子是我记得回想我的博士时代做研究。你读别人的论文，你努力理解它在做什么。然后你抓住他们，你们后来在会议上喝啤酒，你问他们，“所以这篇论文，你在做什么？这篇论文是关于什么的？”

**Andrej Karpathy：** 他们只会告诉你这三句话，完美地捕捉了那篇论文的精髓，完全给了你那个想法。而你不必读那篇论文。只有当你坐在桌边喝啤酒或什么的，他们就像，“噢是的，这篇论文就是，你拿这个想法，你拿那个想法，尝试这个实验，你试了这个东西。”他们有一种方式在对话中完美地表达出来。

**采访者：** 为什么那不是摘要？

**Andrej Karpathy：** 没错。这是从试图解释一个想法的人应该如何更好地制定它的角度来看的。作为一名学生，如果你没有 Karpathy 来阐述一个想法，你给其他学生的建议是什么？

## 学习策略：按需学习与费曼技巧

**采访者：** 如果你在读某人的论文或读书，你采用什么策略来学习你不擅长的领域的感兴趣的材料？

**Andrej Karpathy：** 说实话，我不知道我是否有独特的技巧和窍门。这是一个痛苦的过程。有一件事一直对我很有帮助——我就此发过一条小推文——按需学习相当不错。深度学习（learning depth-wise）。

**Andrej Karpathy：** 我确实觉得你需要交替进行深度学习，按需——你试图实现某个项目，你会从中获得奖励——和广度学习（learning breadth-wise），也就是，“噢，让我们做任何 101，这是你可能需要的所有东西。”

**Andrej Karpathy：** 很多学校就是这样——做广度学习，比如，“噢，相信我，你以后会需要这个，”那种东西。好吧，我相信你。我会学它，因为我想我需要它。但我喜欢那种你会从中获得奖励的学习，你是按需学习。

**Andrej Karpathy：** 我发现非常有帮助的另一件事。这是教育有点无私的一个方面，但向人们解释事情是更深入地学习某些东西的美妙方式。这一直发生在我身上。

**Andrej Karpathy：** 这可能也发生在其他人身上，因为我意识到如果我真的不理解某件事，我就无法解释它。我尝试着，我就像，“噢，我不理解这个。”不得不承认这一点很烦人。

![详细版信息图](/images/karpathy详细版信息图.avif)

**Andrej Karpathy：** 你可以回去确保你理解了它。它填补了你理解的这些空白。它迫使你去面对它们并调和它们。我喜欢重新解释事情，人们也应该更多地这样做。这迫使你操纵知识，并确保你在解释时知道你在说什么。

**采访者：** 这是一个极好的结束语。Andrej，太棒了。谢谢你。